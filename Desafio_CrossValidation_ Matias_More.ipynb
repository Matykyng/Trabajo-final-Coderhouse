{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Proyecto Final\n","by Matías Moré"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["#Librerias\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","sns.set_palette([\"#024059\", \"#66D9CD\", \"#D9923B\", \"#BF726B\", \"#401F1F\"])\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Dataset Fumadores"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Este conjunto de datos consiste en datos biologicos de distintos individuos, recolectados con el objetivo de analizar los efectos y las señales del habito tabaquico. A travez de distintas caracteristicas de estudios de laboratorio como el estado sanguineo, urinario, hepatico, higienico y su relacion con el tabaquismo, se buscara encontrar patrones que determinen biologicamente los parametros a tener en cuenta para identificar a individuos fumadores. Teniendo en cuenta estos datos se realizara una comparacion de estas variables a fin de determinar su utilidad y trascendencia para lograr este fin.\n","\n","Origen del dataset: https://www.kaggle.com/datasets/kukuroo3/body-signal-of-smoking?resource=download"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Definición de objetivo"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["En este trabajo se intenta desarrollar, a partir de los datos proporcionados, un modelo que permita predecir de la forma mas eficiente los individuos que sean fumadores."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Contexto comercial"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Se intenta elaborar una predicción a patir de las historias clínicas, para informar a los profesionales sobre si deben considerar la posibilidad de que sus pacientes sean fumadores y tomarlo en cuenta a la hora de elaborar tratamientos o indicar medicamentos."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Problema comercial"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["¿Es posible, con los datos de los pacientes en nuestra base de datos de historia clinica, encontrar patrones que identifiquen hábitos tabáquicos? ¿En qué grupos de pacientes  debemos incursionar más para la prevención de tabaquismo?"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Contexto analítico"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Se toman 55692 pacientes de la base de datos de historia clinica, a quienes se les identifican los estudios básicos completos y quienes cuentan con la verificacion profesional de si son fumadores o no. Estos datos se pretenden usar para encausar un modelo predictivo de aprendizaje supervisado que nos permita identificar los futuros casos de pacientes a quienes no se les determinó aún si son fumadores o no. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Code Acquisition"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>gender</th>\n","      <th>age</th>\n","      <th>height(cm)</th>\n","      <th>weight(kg)</th>\n","      <th>waist(cm)</th>\n","      <th>eyesight(left)</th>\n","      <th>eyesight(right)</th>\n","      <th>hearing(left)</th>\n","      <th>hearing(right)</th>\n","      <th>...</th>\n","      <th>hemoglobin</th>\n","      <th>Urine protein</th>\n","      <th>serum creatinine</th>\n","      <th>AST</th>\n","      <th>ALT</th>\n","      <th>Gtp</th>\n","      <th>oral</th>\n","      <th>dental caries</th>\n","      <th>tartar</th>\n","      <th>smoking</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>F</td>\n","      <td>40</td>\n","      <td>155</td>\n","      <td>60</td>\n","      <td>81.3</td>\n","      <td>1.2</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>12.9</td>\n","      <td>1.0</td>\n","      <td>0.7</td>\n","      <td>18.0</td>\n","      <td>19.0</td>\n","      <td>27.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>F</td>\n","      <td>40</td>\n","      <td>160</td>\n","      <td>60</td>\n","      <td>81.0</td>\n","      <td>0.8</td>\n","      <td>0.6</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>12.7</td>\n","      <td>1.0</td>\n","      <td>0.6</td>\n","      <td>22.0</td>\n","      <td>19.0</td>\n","      <td>18.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>M</td>\n","      <td>55</td>\n","      <td>170</td>\n","      <td>60</td>\n","      <td>80.0</td>\n","      <td>0.8</td>\n","      <td>0.8</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>15.8</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>21.0</td>\n","      <td>16.0</td>\n","      <td>22.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","      <td>N</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>M</td>\n","      <td>40</td>\n","      <td>165</td>\n","      <td>70</td>\n","      <td>88.0</td>\n","      <td>1.5</td>\n","      <td>1.5</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>14.7</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>19.0</td>\n","      <td>26.0</td>\n","      <td>18.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>F</td>\n","      <td>40</td>\n","      <td>155</td>\n","      <td>60</td>\n","      <td>86.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>12.5</td>\n","      <td>1.0</td>\n","      <td>0.6</td>\n","      <td>16.0</td>\n","      <td>14.0</td>\n","      <td>22.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","      <td>N</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>55687</th>\n","      <td>55676</td>\n","      <td>F</td>\n","      <td>40</td>\n","      <td>170</td>\n","      <td>65</td>\n","      <td>75.0</td>\n","      <td>0.9</td>\n","      <td>0.9</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>12.3</td>\n","      <td>1.0</td>\n","      <td>0.6</td>\n","      <td>14.0</td>\n","      <td>7.0</td>\n","      <td>10.0</td>\n","      <td>Y</td>\n","      <td>1</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>55688</th>\n","      <td>55681</td>\n","      <td>F</td>\n","      <td>45</td>\n","      <td>160</td>\n","      <td>50</td>\n","      <td>70.0</td>\n","      <td>1.2</td>\n","      <td>1.2</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>14.0</td>\n","      <td>1.0</td>\n","      <td>0.9</td>\n","      <td>20.0</td>\n","      <td>12.0</td>\n","      <td>14.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>55689</th>\n","      <td>55683</td>\n","      <td>F</td>\n","      <td>55</td>\n","      <td>160</td>\n","      <td>50</td>\n","      <td>68.5</td>\n","      <td>1.0</td>\n","      <td>1.2</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>12.4</td>\n","      <td>1.0</td>\n","      <td>0.5</td>\n","      <td>17.0</td>\n","      <td>11.0</td>\n","      <td>12.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","      <td>N</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>55690</th>\n","      <td>55684</td>\n","      <td>M</td>\n","      <td>60</td>\n","      <td>165</td>\n","      <td>60</td>\n","      <td>78.0</td>\n","      <td>0.8</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>14.4</td>\n","      <td>1.0</td>\n","      <td>0.7</td>\n","      <td>20.0</td>\n","      <td>19.0</td>\n","      <td>18.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","      <td>N</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>55691</th>\n","      <td>55691</td>\n","      <td>M</td>\n","      <td>55</td>\n","      <td>160</td>\n","      <td>65</td>\n","      <td>85.0</td>\n","      <td>0.9</td>\n","      <td>0.7</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>15.0</td>\n","      <td>1.0</td>\n","      <td>0.8</td>\n","      <td>26.0</td>\n","      <td>29.0</td>\n","      <td>41.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","      <td>Y</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>55692 rows × 27 columns</p>\n","</div>"],"text/plain":["          ID gender  age  height(cm)  weight(kg)  waist(cm)  eyesight(left)  \\\n","0          0      F   40         155          60       81.3             1.2   \n","1          1      F   40         160          60       81.0             0.8   \n","2          2      M   55         170          60       80.0             0.8   \n","3          3      M   40         165          70       88.0             1.5   \n","4          4      F   40         155          60       86.0             1.0   \n","...      ...    ...  ...         ...         ...        ...             ...   \n","55687  55676      F   40         170          65       75.0             0.9   \n","55688  55681      F   45         160          50       70.0             1.2   \n","55689  55683      F   55         160          50       68.5             1.0   \n","55690  55684      M   60         165          60       78.0             0.8   \n","55691  55691      M   55         160          65       85.0             0.9   \n","\n","       eyesight(right)  hearing(left)  hearing(right)  ...  hemoglobin  \\\n","0                  1.0            1.0             1.0  ...        12.9   \n","1                  0.6            1.0             1.0  ...        12.7   \n","2                  0.8            1.0             1.0  ...        15.8   \n","3                  1.5            1.0             1.0  ...        14.7   \n","4                  1.0            1.0             1.0  ...        12.5   \n","...                ...            ...             ...  ...         ...   \n","55687              0.9            1.0             1.0  ...        12.3   \n","55688              1.2            1.0             1.0  ...        14.0   \n","55689              1.2            1.0             1.0  ...        12.4   \n","55690              1.0            1.0             1.0  ...        14.4   \n","55691              0.7            1.0             1.0  ...        15.0   \n","\n","       Urine protein  serum creatinine   AST   ALT   Gtp  oral  dental caries  \\\n","0                1.0               0.7  18.0  19.0  27.0     Y              0   \n","1                1.0               0.6  22.0  19.0  18.0     Y              0   \n","2                1.0               1.0  21.0  16.0  22.0     Y              0   \n","3                1.0               1.0  19.0  26.0  18.0     Y              0   \n","4                1.0               0.6  16.0  14.0  22.0     Y              0   \n","...              ...               ...   ...   ...   ...   ...            ...   \n","55687            1.0               0.6  14.0   7.0  10.0     Y              1   \n","55688            1.0               0.9  20.0  12.0  14.0     Y              0   \n","55689            1.0               0.5  17.0  11.0  12.0     Y              0   \n","55690            1.0               0.7  20.0  19.0  18.0     Y              0   \n","55691            1.0               0.8  26.0  29.0  41.0     Y              0   \n","\n","       tartar  smoking  \n","0           Y        0  \n","1           Y        0  \n","2           N        1  \n","3           Y        0  \n","4           N        0  \n","...       ...      ...  \n","55687       Y        0  \n","55688       Y        0  \n","55689       N        0  \n","55690       N        0  \n","55691       Y        1  \n","\n","[55692 rows x 27 columns]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["smoke_data = pd.read_csv(\"/workspaces/CoderPosta/Data/smoking.csv\")\n","smoke_data"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["df = smoke_data.copy()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Limpieza de Datos"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Nulos y Duplicados"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Se verifica que no hay duplicados ni nulos."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ID                     0\n","dental caries          0\n","oral                   0\n","Gtp                    0\n","ALT                    0\n","AST                    0\n","serum creatinine       0\n","Urine protein          0\n","hemoglobin             0\n","LDL                    0\n","HDL                    0\n","triglyceride           0\n","tartar                 0\n","Cholesterol            0\n","relaxation             0\n","systolic               0\n","hearing(right)         0\n","hearing(left)          0\n","eyesight(right)        0\n","eyesight(left)         0\n","waist(cm)              0\n","weight(kg)             0\n","height(cm)             0\n","age                    0\n","gender                 0\n","fasting blood sugar    0\n","smoking                0\n","dtype: int64\n","Cant. de Duplicados  0\n"]}],"source":["print(df.isna().sum().sort_values())\n","print('Cant. de Duplicados ' ,df.duplicated().sum())"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Agrupacion de Datos"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["ID                       int64\n","gender                  object\n","age                      int64\n","height(cm)               int64\n","weight(kg)               int64\n","waist(cm)              float64\n","eyesight(left)         float64\n","eyesight(right)        float64\n","hearing(left)          float64\n","hearing(right)         float64\n","systolic               float64\n","relaxation             float64\n","fasting blood sugar    float64\n","Cholesterol            float64\n","triglyceride           float64\n","HDL                    float64\n","LDL                    float64\n","hemoglobin             float64\n","Urine protein          float64\n","serum creatinine       float64\n","AST                    float64\n","ALT                    float64\n","Gtp                    float64\n","oral                    object\n","dental caries            int64\n","tartar                  object\n","smoking                  int64\n","dtype: object"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df.dtypes"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Datos  Categóricos"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Categoric Col ['gender', 'oral', 'tartar']\n"]}],"source":["categoric_col = []\n","for col in df.columns:\n","\tif (df[col].nunique()<10) and (df[col].dtype==\"object\"):\n","\t\tcategoric_col.append(col) \n","  \n","print(\"Categoric Col {}\".format(categoric_col))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Datos Continuos"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Cont Col ['hearing(left)', 'relaxation', 'eyesight(right)', 'age', 'fasting blood sugar', 'serum creatinine', 'waist(cm)', 'eyesight(left)', 'smoking', 'height(cm)', 'Urine protein', 'hearing(right)', 'Gtp', 'hemoglobin', 'triglyceride', 'HDL', 'systolic', 'ID', 'LDL', 'AST', 'ALT', 'Cholesterol', 'dental caries', 'weight(kg)']\n"]}],"source":["Numeric=list(set(df.columns)- set(categoric_col))\n","print(\"Cont Col {}\".format(Numeric))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Se identifica que la columna \"oral\" posee una sola variable unica."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["oral                       1\n","smoking                    2\n","gender                     2\n","dental caries              2\n","hearing(left)              2\n","hearing(right)             2\n","tartar                     2\n","Urine protein              6\n","height(cm)                13\n","age                       14\n","eyesight(right)           17\n","eyesight(left)            19\n","weight(kg)                22\n","serum creatinine          38\n","relaxation                95\n","HDL                      126\n","systolic                 130\n","hemoglobin               145\n","AST                      219\n","ALT                      245\n","fasting blood sugar      276\n","Cholesterol              286\n","LDL                      289\n","triglyceride             390\n","Gtp                      488\n","waist(cm)                566\n","ID                     55692\n","dtype: int64"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["df.nunique().sort_values()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Se elimina la variable \"oral\" por su irrelevancia para el modelo."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>gender</th>\n","      <th>age</th>\n","      <th>height(cm)</th>\n","      <th>weight(kg)</th>\n","      <th>waist(cm)</th>\n","      <th>eyesight(left)</th>\n","      <th>eyesight(right)</th>\n","      <th>hearing(left)</th>\n","      <th>hearing(right)</th>\n","      <th>...</th>\n","      <th>LDL</th>\n","      <th>hemoglobin</th>\n","      <th>Urine protein</th>\n","      <th>serum creatinine</th>\n","      <th>AST</th>\n","      <th>ALT</th>\n","      <th>Gtp</th>\n","      <th>dental caries</th>\n","      <th>tartar</th>\n","      <th>smoking</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>F</td>\n","      <td>40</td>\n","      <td>155</td>\n","      <td>60</td>\n","      <td>81.3</td>\n","      <td>1.2</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>126.0</td>\n","      <td>12.9</td>\n","      <td>1.0</td>\n","      <td>0.7</td>\n","      <td>18.0</td>\n","      <td>19.0</td>\n","      <td>27.0</td>\n","      <td>0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>F</td>\n","      <td>40</td>\n","      <td>160</td>\n","      <td>60</td>\n","      <td>81.0</td>\n","      <td>0.8</td>\n","      <td>0.6</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>127.0</td>\n","      <td>12.7</td>\n","      <td>1.0</td>\n","      <td>0.6</td>\n","      <td>22.0</td>\n","      <td>19.0</td>\n","      <td>18.0</td>\n","      <td>0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>M</td>\n","      <td>55</td>\n","      <td>170</td>\n","      <td>60</td>\n","      <td>80.0</td>\n","      <td>0.8</td>\n","      <td>0.8</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>151.0</td>\n","      <td>15.8</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>21.0</td>\n","      <td>16.0</td>\n","      <td>22.0</td>\n","      <td>0</td>\n","      <td>N</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>M</td>\n","      <td>40</td>\n","      <td>165</td>\n","      <td>70</td>\n","      <td>88.0</td>\n","      <td>1.5</td>\n","      <td>1.5</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>226.0</td>\n","      <td>14.7</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>19.0</td>\n","      <td>26.0</td>\n","      <td>18.0</td>\n","      <td>0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>F</td>\n","      <td>40</td>\n","      <td>155</td>\n","      <td>60</td>\n","      <td>86.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>107.0</td>\n","      <td>12.5</td>\n","      <td>1.0</td>\n","      <td>0.6</td>\n","      <td>16.0</td>\n","      <td>14.0</td>\n","      <td>22.0</td>\n","      <td>0</td>\n","      <td>N</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 26 columns</p>\n","</div>"],"text/plain":["   ID gender  age  height(cm)  weight(kg)  waist(cm)  eyesight(left)  \\\n","0   0      F   40         155          60       81.3             1.2   \n","1   1      F   40         160          60       81.0             0.8   \n","2   2      M   55         170          60       80.0             0.8   \n","3   3      M   40         165          70       88.0             1.5   \n","4   4      F   40         155          60       86.0             1.0   \n","\n","   eyesight(right)  hearing(left)  hearing(right)  ...    LDL  hemoglobin  \\\n","0              1.0            1.0             1.0  ...  126.0        12.9   \n","1              0.6            1.0             1.0  ...  127.0        12.7   \n","2              0.8            1.0             1.0  ...  151.0        15.8   \n","3              1.5            1.0             1.0  ...  226.0        14.7   \n","4              1.0            1.0             1.0  ...  107.0        12.5   \n","\n","   Urine protein  serum creatinine   AST   ALT   Gtp  dental caries  tartar  \\\n","0            1.0               0.7  18.0  19.0  27.0              0       Y   \n","1            1.0               0.6  22.0  19.0  18.0              0       Y   \n","2            1.0               1.0  21.0  16.0  22.0              0       N   \n","3            1.0               1.0  19.0  26.0  18.0              0       Y   \n","4            1.0               0.6  16.0  14.0  22.0              0       N   \n","\n","   smoking  \n","0        0  \n","1        0  \n","2        1  \n","3        0  \n","4        0  \n","\n","[5 rows x 26 columns]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["df.drop(labels=\"oral\", axis=1, inplace=True)\n","df.head(5)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Por lo que se observa en los resultados las columnas \"urine protein\", \"hearing(right)\", \"hearing(left)\"y \"dental caries\" no producen informacion valiosa para el modelo asi que se eliminan del \"ds\""]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>gender</th>\n","      <th>age</th>\n","      <th>height(cm)</th>\n","      <th>weight(kg)</th>\n","      <th>waist(cm)</th>\n","      <th>eyesight(left)</th>\n","      <th>eyesight(right)</th>\n","      <th>systolic</th>\n","      <th>relaxation</th>\n","      <th>fasting blood sugar</th>\n","      <th>...</th>\n","      <th>triglyceride</th>\n","      <th>HDL</th>\n","      <th>LDL</th>\n","      <th>hemoglobin</th>\n","      <th>serum creatinine</th>\n","      <th>AST</th>\n","      <th>ALT</th>\n","      <th>Gtp</th>\n","      <th>tartar</th>\n","      <th>smoking</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>F</td>\n","      <td>40</td>\n","      <td>155</td>\n","      <td>60</td>\n","      <td>81.3</td>\n","      <td>1.2</td>\n","      <td>1.0</td>\n","      <td>114.0</td>\n","      <td>73.0</td>\n","      <td>94.0</td>\n","      <td>...</td>\n","      <td>82.0</td>\n","      <td>73.0</td>\n","      <td>126.0</td>\n","      <td>12.9</td>\n","      <td>0.7</td>\n","      <td>18.0</td>\n","      <td>19.0</td>\n","      <td>27.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>F</td>\n","      <td>40</td>\n","      <td>160</td>\n","      <td>60</td>\n","      <td>81.0</td>\n","      <td>0.8</td>\n","      <td>0.6</td>\n","      <td>119.0</td>\n","      <td>70.0</td>\n","      <td>130.0</td>\n","      <td>...</td>\n","      <td>115.0</td>\n","      <td>42.0</td>\n","      <td>127.0</td>\n","      <td>12.7</td>\n","      <td>0.6</td>\n","      <td>22.0</td>\n","      <td>19.0</td>\n","      <td>18.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>M</td>\n","      <td>55</td>\n","      <td>170</td>\n","      <td>60</td>\n","      <td>80.0</td>\n","      <td>0.8</td>\n","      <td>0.8</td>\n","      <td>138.0</td>\n","      <td>86.0</td>\n","      <td>89.0</td>\n","      <td>...</td>\n","      <td>182.0</td>\n","      <td>55.0</td>\n","      <td>151.0</td>\n","      <td>15.8</td>\n","      <td>1.0</td>\n","      <td>21.0</td>\n","      <td>16.0</td>\n","      <td>22.0</td>\n","      <td>N</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>M</td>\n","      <td>40</td>\n","      <td>165</td>\n","      <td>70</td>\n","      <td>88.0</td>\n","      <td>1.5</td>\n","      <td>1.5</td>\n","      <td>100.0</td>\n","      <td>60.0</td>\n","      <td>96.0</td>\n","      <td>...</td>\n","      <td>254.0</td>\n","      <td>45.0</td>\n","      <td>226.0</td>\n","      <td>14.7</td>\n","      <td>1.0</td>\n","      <td>19.0</td>\n","      <td>26.0</td>\n","      <td>18.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>F</td>\n","      <td>40</td>\n","      <td>155</td>\n","      <td>60</td>\n","      <td>86.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>120.0</td>\n","      <td>74.0</td>\n","      <td>80.0</td>\n","      <td>...</td>\n","      <td>74.0</td>\n","      <td>62.0</td>\n","      <td>107.0</td>\n","      <td>12.5</td>\n","      <td>0.6</td>\n","      <td>16.0</td>\n","      <td>14.0</td>\n","      <td>22.0</td>\n","      <td>N</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>55687</th>\n","      <td>F</td>\n","      <td>40</td>\n","      <td>170</td>\n","      <td>65</td>\n","      <td>75.0</td>\n","      <td>0.9</td>\n","      <td>0.9</td>\n","      <td>110.0</td>\n","      <td>68.0</td>\n","      <td>89.0</td>\n","      <td>...</td>\n","      <td>99.0</td>\n","      <td>75.0</td>\n","      <td>118.0</td>\n","      <td>12.3</td>\n","      <td>0.6</td>\n","      <td>14.0</td>\n","      <td>7.0</td>\n","      <td>10.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>55688</th>\n","      <td>F</td>\n","      <td>45</td>\n","      <td>160</td>\n","      <td>50</td>\n","      <td>70.0</td>\n","      <td>1.2</td>\n","      <td>1.2</td>\n","      <td>101.0</td>\n","      <td>62.0</td>\n","      <td>89.0</td>\n","      <td>...</td>\n","      <td>69.0</td>\n","      <td>73.0</td>\n","      <td>79.0</td>\n","      <td>14.0</td>\n","      <td>0.9</td>\n","      <td>20.0</td>\n","      <td>12.0</td>\n","      <td>14.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>55689</th>\n","      <td>F</td>\n","      <td>55</td>\n","      <td>160</td>\n","      <td>50</td>\n","      <td>68.5</td>\n","      <td>1.0</td>\n","      <td>1.2</td>\n","      <td>117.0</td>\n","      <td>72.0</td>\n","      <td>88.0</td>\n","      <td>...</td>\n","      <td>77.0</td>\n","      <td>79.0</td>\n","      <td>63.0</td>\n","      <td>12.4</td>\n","      <td>0.5</td>\n","      <td>17.0</td>\n","      <td>11.0</td>\n","      <td>12.0</td>\n","      <td>N</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>55690</th>\n","      <td>M</td>\n","      <td>60</td>\n","      <td>165</td>\n","      <td>60</td>\n","      <td>78.0</td>\n","      <td>0.8</td>\n","      <td>1.0</td>\n","      <td>133.0</td>\n","      <td>76.0</td>\n","      <td>107.0</td>\n","      <td>...</td>\n","      <td>79.0</td>\n","      <td>48.0</td>\n","      <td>146.0</td>\n","      <td>14.4</td>\n","      <td>0.7</td>\n","      <td>20.0</td>\n","      <td>19.0</td>\n","      <td>18.0</td>\n","      <td>N</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>55691</th>\n","      <td>M</td>\n","      <td>55</td>\n","      <td>160</td>\n","      <td>65</td>\n","      <td>85.0</td>\n","      <td>0.9</td>\n","      <td>0.7</td>\n","      <td>124.0</td>\n","      <td>75.0</td>\n","      <td>82.0</td>\n","      <td>...</td>\n","      <td>142.0</td>\n","      <td>34.0</td>\n","      <td>150.0</td>\n","      <td>15.0</td>\n","      <td>0.8</td>\n","      <td>26.0</td>\n","      <td>29.0</td>\n","      <td>41.0</td>\n","      <td>Y</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>55692 rows × 21 columns</p>\n","</div>"],"text/plain":["      gender  age  height(cm)  weight(kg)  waist(cm)  eyesight(left)  \\\n","0          F   40         155          60       81.3             1.2   \n","1          F   40         160          60       81.0             0.8   \n","2          M   55         170          60       80.0             0.8   \n","3          M   40         165          70       88.0             1.5   \n","4          F   40         155          60       86.0             1.0   \n","...      ...  ...         ...         ...        ...             ...   \n","55687      F   40         170          65       75.0             0.9   \n","55688      F   45         160          50       70.0             1.2   \n","55689      F   55         160          50       68.5             1.0   \n","55690      M   60         165          60       78.0             0.8   \n","55691      M   55         160          65       85.0             0.9   \n","\n","       eyesight(right)  systolic  relaxation  fasting blood sugar  ...  \\\n","0                  1.0     114.0        73.0                 94.0  ...   \n","1                  0.6     119.0        70.0                130.0  ...   \n","2                  0.8     138.0        86.0                 89.0  ...   \n","3                  1.5     100.0        60.0                 96.0  ...   \n","4                  1.0     120.0        74.0                 80.0  ...   \n","...                ...       ...         ...                  ...  ...   \n","55687              0.9     110.0        68.0                 89.0  ...   \n","55688              1.2     101.0        62.0                 89.0  ...   \n","55689              1.2     117.0        72.0                 88.0  ...   \n","55690              1.0     133.0        76.0                107.0  ...   \n","55691              0.7     124.0        75.0                 82.0  ...   \n","\n","       triglyceride   HDL    LDL  hemoglobin  serum creatinine   AST   ALT  \\\n","0              82.0  73.0  126.0        12.9               0.7  18.0  19.0   \n","1             115.0  42.0  127.0        12.7               0.6  22.0  19.0   \n","2             182.0  55.0  151.0        15.8               1.0  21.0  16.0   \n","3             254.0  45.0  226.0        14.7               1.0  19.0  26.0   \n","4              74.0  62.0  107.0        12.5               0.6  16.0  14.0   \n","...             ...   ...    ...         ...               ...   ...   ...   \n","55687          99.0  75.0  118.0        12.3               0.6  14.0   7.0   \n","55688          69.0  73.0   79.0        14.0               0.9  20.0  12.0   \n","55689          77.0  79.0   63.0        12.4               0.5  17.0  11.0   \n","55690          79.0  48.0  146.0        14.4               0.7  20.0  19.0   \n","55691         142.0  34.0  150.0        15.0               0.8  26.0  29.0   \n","\n","        Gtp  tartar smoking  \n","0      27.0       Y       0  \n","1      18.0       Y       0  \n","2      22.0       N       1  \n","3      18.0       Y       0  \n","4      22.0       N       0  \n","...     ...     ...     ...  \n","55687  10.0       Y       0  \n","55688  14.0       Y       0  \n","55689  12.0       N       0  \n","55690  18.0       N       0  \n","55691  41.0       Y       1  \n","\n","[55692 rows x 21 columns]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["df.drop(labels=[\"Urine protein\", \"hearing(right)\", \"hearing(left)\", \"dental caries\", \"ID\"], axis=1, inplace=True)\n","df"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Feature Creation"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Se crea la columna Indice de Masa Corporal (IMC) y se decide probar el modelo eliminando sus columnas correlacionadas"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["df[\"height(M)\"] = df[\"height(cm)\"] / 100\n","df[\"IMC\"] = df[\"weight(kg)\"] / (df[\"height(M)\"] ** 2)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Se crea la columna Tension Arterial Media (TAM) y se aplica lo mismo que en IMC"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["df['TAM'] = (df['systolic'] + 0.5 * df['relaxation']) / 2\n","df = df.drop([\"systolic\",\"relaxation\"], axis = 1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Se realiza la codificacion de los datos categoricos"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","for col in df.columns:\n","    if df[col].dtype == 'object':\n","        df[col] = le.fit_transform(df[col])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Feature Scaling"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import RobustScaler\n","from sklearn.preprocessing import PowerTransformer"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["df_scaled = df.drop([\"gender\",\"tartar\",\"smoking\"], axis=1)\n","scaling = PowerTransformer().fit_transform(df_scaled)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["df_scaled = pd.DataFrame(scaling, columns=df_scaled.columns)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["df = df_scaled.join(df[['gender', 'tartar', \"smoking\"]])"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install -q lightgbm"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["se realiza la subdivision de x e y en archivo con limpieza y sin limpieza"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["x = df.drop(\"smoking\", axis=1)      \n","y = df['smoking']"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Manejo de outlayers"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["from sklearn.ensemble import IsolationForest\n","\n","# Identificar outliers en el dataset de entrenamiento\n","iso = IsolationForest(contamination=0.1) # contamination = proporción de outliers esperada\n","yhat = iso.fit_predict(x)\n","\n","# Seleccionar todas las filas que no son outliers\n","mask = yhat != -1 # filtro\n","x = x.loc[mask, :]\n","y = y.loc[mask]"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>height(cm)</th>\n","      <th>weight(kg)</th>\n","      <th>waist(cm)</th>\n","      <th>eyesight(left)</th>\n","      <th>eyesight(right)</th>\n","      <th>fasting blood sugar</th>\n","      <th>Cholesterol</th>\n","      <th>triglyceride</th>\n","      <th>HDL</th>\n","      <th>...</th>\n","      <th>hemoglobin</th>\n","      <th>serum creatinine</th>\n","      <th>AST</th>\n","      <th>ALT</th>\n","      <th>Gtp</th>\n","      <th>height(M)</th>\n","      <th>IMC</th>\n","      <th>TAM</th>\n","      <th>gender</th>\n","      <th>tartar</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.297269</td>\n","      <td>-1.050222</td>\n","      <td>-0.384530</td>\n","      <td>-0.040085</td>\n","      <td>0.611271</td>\n","      <td>0.111054</td>\n","      <td>-0.125467</td>\n","      <td>0.541670</td>\n","      <td>-0.527427</td>\n","      <td>1.103291</td>\n","      <td>...</td>\n","      <td>-1.126847</td>\n","      <td>-0.940852</td>\n","      <td>-0.864862</td>\n","      <td>-0.211448</td>\n","      <td>0.054721</td>\n","      <td>-1.049889</td>\n","      <td>0.322204</td>\n","      <td>-0.470663</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.297269</td>\n","      <td>-0.528799</td>\n","      <td>-0.384530</td>\n","      <td>-0.072702</td>\n","      <td>-0.489408</td>\n","      <td>-1.155756</td>\n","      <td>1.690480</td>\n","      <td>-0.079764</td>\n","      <td>0.103300</td>\n","      <td>-1.132692</td>\n","      <td>...</td>\n","      <td>-1.233483</td>\n","      <td>-1.567788</td>\n","      <td>-0.140581</td>\n","      <td>-0.211448</td>\n","      <td>-0.627463</td>\n","      <td>-0.529365</td>\n","      <td>-0.124719</td>\n","      <td>-0.261007</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.905592</td>\n","      <td>0.564979</td>\n","      <td>-0.384530</td>\n","      <td>-0.182056</td>\n","      <td>-0.489408</td>\n","      <td>-0.475679</td>\n","      <td>-0.561403</td>\n","      <td>1.221340</td>\n","      <td>0.943687</td>\n","      <td>-0.033459</td>\n","      <td>...</td>\n","      <td>0.745089</td>\n","      <td>0.639395</td>\n","      <td>-0.298108</td>\n","      <td>-0.564867</td>\n","      <td>-0.275446</td>\n","      <td>0.564103</td>\n","      <td>-1.000805</td>\n","      <td>1.204194</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-0.297269</td>\n","      <td>0.009596</td>\n","      <td>0.413851</td>\n","      <td>0.666754</td>\n","      <td>1.275418</td>\n","      <td>1.289465</td>\n","      <td>0.030444</td>\n","      <td>2.996936</td>\n","      <td>1.542069</td>\n","      <td>-0.849996</td>\n","      <td>...</td>\n","      <td>-0.039193</td>\n","      <td>0.639395</td>\n","      <td>-0.657974</td>\n","      <td>0.378473</td>\n","      <td>-0.627463</td>\n","      <td>0.008554</td>\n","      <td>0.524383</td>\n","      <td>-1.812592</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.297269</td>\n","      <td>-1.050222</td>\n","      <td>-0.384530</td>\n","      <td>0.459942</td>\n","      <td>0.097565</td>\n","      <td>0.111054</td>\n","      <td>-1.557496</td>\n","      <td>-0.306532</td>\n","      <td>-0.720712</td>\n","      <td>0.449791</td>\n","      <td>...</td>\n","      <td>-1.337390</td>\n","      <td>-1.567788</td>\n","      <td>-1.347831</td>\n","      <td>-0.854762</td>\n","      <td>-0.275446</td>\n","      <td>-1.049889</td>\n","      <td>0.322204</td>\n","      <td>-0.085278</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>55686</th>\n","      <td>1.277257</td>\n","      <td>-1.554690</td>\n","      <td>-1.332362</td>\n","      <td>-0.744104</td>\n","      <td>0.097565</td>\n","      <td>0.624829</td>\n","      <td>-0.965864</td>\n","      <td>-0.451334</td>\n","      <td>-1.354699</td>\n","      <td>-0.260813</td>\n","      <td>...</td>\n","      <td>-1.285776</td>\n","      <td>-0.368763</td>\n","      <td>0.377833</td>\n","      <td>-0.015611</td>\n","      <td>-1.110260</td>\n","      <td>-1.553257</td>\n","      <td>-0.505758</td>\n","      <td>-1.672152</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>55688</th>\n","      <td>0.120440</td>\n","      <td>-0.528799</td>\n","      <td>-1.332362</td>\n","      <td>-1.333799</td>\n","      <td>0.611271</td>\n","      <td>0.624829</td>\n","      <td>-0.561403</td>\n","      <td>-0.839770</td>\n","      <td>-0.852907</td>\n","      <td>1.103291</td>\n","      <td>...</td>\n","      <td>-0.490168</td>\n","      <td>0.155945</td>\n","      <td>-0.469885</td>\n","      <td>-1.206312</td>\n","      <td>-1.110260</td>\n","      <td>-0.529365</td>\n","      <td>-1.453434</td>\n","      <td>-1.672152</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>55689</th>\n","      <td>0.905592</td>\n","      <td>-0.528799</td>\n","      <td>-1.332362</td>\n","      <td>-1.516599</td>\n","      <td>0.097565</td>\n","      <td>0.624829</td>\n","      <td>-0.657459</td>\n","      <td>-1.088115</td>\n","      <td>-0.645787</td>\n","      <td>1.417086</td>\n","      <td>...</td>\n","      <td>-1.388329</td>\n","      <td>-2.258780</td>\n","      <td>-1.093576</td>\n","      <td>-1.412794</td>\n","      <td>-1.430352</td>\n","      <td>-0.529365</td>\n","      <td>-1.453434</td>\n","      <td>-0.320390</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>55690</th>\n","      <td>1.277257</td>\n","      <td>0.009596</td>\n","      <td>-0.384530</td>\n","      <td>-0.403754</td>\n","      <td>-0.489408</td>\n","      <td>0.111054</td>\n","      <td>0.739741</td>\n","      <td>0.410167</td>\n","      <td>-0.597508</td>\n","      <td>-0.586445</td>\n","      <td>...</td>\n","      <td>-0.236945</td>\n","      <td>-0.940852</td>\n","      <td>-0.469885</td>\n","      <td>-0.211448</td>\n","      <td>-0.627463</td>\n","      <td>0.008554</td>\n","      <td>-0.565665</td>\n","      <td>0.690330</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>55691</th>\n","      <td>0.905592</td>\n","      <td>-0.528799</td>\n","      <td>0.030394</td>\n","      <td>0.355234</td>\n","      <td>-0.185649</td>\n","      <td>-0.802468</td>\n","      <td>-1.308076</td>\n","      <td>0.489296</td>\n","      <td>0.491671</td>\n","      <td>-2.004351</td>\n","      <td>...</td>\n","      <td>0.165388</td>\n","      <td>-0.368763</td>\n","      <td>0.377833</td>\n","      <td>0.567831</td>\n","      <td>0.644218</td>\n","      <td>-0.529365</td>\n","      <td>0.437326</td>\n","      <td>0.171758</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>50122 rows × 21 columns</p>\n","</div>"],"text/plain":["            age  height(cm)  weight(kg)  waist(cm)  eyesight(left)  \\\n","0     -0.297269   -1.050222   -0.384530  -0.040085        0.611271   \n","1     -0.297269   -0.528799   -0.384530  -0.072702       -0.489408   \n","2      0.905592    0.564979   -0.384530  -0.182056       -0.489408   \n","3     -0.297269    0.009596    0.413851   0.666754        1.275418   \n","4     -0.297269   -1.050222   -0.384530   0.459942        0.097565   \n","...         ...         ...         ...        ...             ...   \n","55686  1.277257   -1.554690   -1.332362  -0.744104        0.097565   \n","55688  0.120440   -0.528799   -1.332362  -1.333799        0.611271   \n","55689  0.905592   -0.528799   -1.332362  -1.516599        0.097565   \n","55690  1.277257    0.009596   -0.384530  -0.403754       -0.489408   \n","55691  0.905592   -0.528799    0.030394   0.355234       -0.185649   \n","\n","       eyesight(right)  fasting blood sugar  Cholesterol  triglyceride  \\\n","0             0.111054            -0.125467     0.541670     -0.527427   \n","1            -1.155756             1.690480    -0.079764      0.103300   \n","2            -0.475679            -0.561403     1.221340      0.943687   \n","3             1.289465             0.030444     2.996936      1.542069   \n","4             0.111054            -1.557496    -0.306532     -0.720712   \n","...                ...                  ...          ...           ...   \n","55686         0.624829            -0.965864    -0.451334     -1.354699   \n","55688         0.624829            -0.561403    -0.839770     -0.852907   \n","55689         0.624829            -0.657459    -1.088115     -0.645787   \n","55690         0.111054             0.739741     0.410167     -0.597508   \n","55691        -0.802468            -1.308076     0.489296      0.491671   \n","\n","            HDL  ...  hemoglobin  serum creatinine       AST       ALT  \\\n","0      1.103291  ...   -1.126847         -0.940852 -0.864862 -0.211448   \n","1     -1.132692  ...   -1.233483         -1.567788 -0.140581 -0.211448   \n","2     -0.033459  ...    0.745089          0.639395 -0.298108 -0.564867   \n","3     -0.849996  ...   -0.039193          0.639395 -0.657974  0.378473   \n","4      0.449791  ...   -1.337390         -1.567788 -1.347831 -0.854762   \n","...         ...  ...         ...               ...       ...       ...   \n","55686 -0.260813  ...   -1.285776         -0.368763  0.377833 -0.015611   \n","55688  1.103291  ...   -0.490168          0.155945 -0.469885 -1.206312   \n","55689  1.417086  ...   -1.388329         -2.258780 -1.093576 -1.412794   \n","55690 -0.586445  ...   -0.236945         -0.940852 -0.469885 -0.211448   \n","55691 -2.004351  ...    0.165388         -0.368763  0.377833  0.567831   \n","\n","            Gtp  height(M)       IMC       TAM  gender  tartar  \n","0      0.054721  -1.049889  0.322204 -0.470663       0       1  \n","1     -0.627463  -0.529365 -0.124719 -0.261007       0       1  \n","2     -0.275446   0.564103 -1.000805  1.204194       1       0  \n","3     -0.627463   0.008554  0.524383 -1.812592       1       1  \n","4     -0.275446  -1.049889  0.322204 -0.085278       0       0  \n","...         ...        ...       ...       ...     ...     ...  \n","55686 -1.110260  -1.553257 -0.505758 -1.672152       0       1  \n","55688 -1.110260  -0.529365 -1.453434 -1.672152       0       1  \n","55689 -1.430352  -0.529365 -1.453434 -0.320390       0       0  \n","55690 -0.627463   0.008554 -0.565665  0.690330       1       0  \n","55691  0.644218  -0.529365  0.437326  0.171758       1       1  \n","\n","[50122 rows x 21 columns]"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["x"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Feature Selection"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# numpy and pandas para data manipulation\n","import pandas as pd\n","import numpy as np\n","# model usado para feature importances\n","import lightgbm as lgb\n","# Utilidad para hacer separacion en train y test\n","from sklearn.model_selection import train_test_split\n","# visualizaciones\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","# manejo de memoria\n","import gc\n","# utilidades\n","from itertools import chain\n","class FeatureSelector():\n","    \"\"\"\n","    Class para desarrollar  feature selection para algoritmos de machine learning o data preprocessing. \n","    Implementa 5 distintos metodos para identificar features a remover\n","        1. Encuentra las columnas con cierto porcentaje de nulos mayor a un threshold\n","        2. Encuentra las columnas con solo un unico valor\n","        3. Encuentra variables con colinealidad con una correlacion mayor a un valor especificado\n","        4. Encuentra los features con  0.0 en feature importance usando una gradient boosting machine (gbm)\n","        5. Encuentra los valores bajos de feature importance que no ocntribuyen a la cumulative feature importance de la gbm\n","    Parametros\n","    --------\n","        data : dataframe\n","            Un dataset con observaciones y columnas ne las filas\n","        labels : array or series, default = None\n","            Arreglo de labels para entrenar el algoritmo de machine learning model para encontrar las feature importances. \n","            Pueden ser labels binarios (si se trata de 'classification') o valores continuos (si la tarea es 'regression').\n","            Si no se proveen labels el feature importance no estara disponible.\n","    Attributos\n","    --------\n","    ops : dict\n","        Dictionary con las operaciones a correr y los features identificados para remocion\n","    missing_stats : dataframe\n","        La fraccion de missing values de todos los features \n","    record_missing : dataframe\n","        La fracción de valores perdidos para entidades con fracción faltante por encima del umbral\n","    unique_stats : dataframe\n","        Numero de valores unicos para todos los features\n","    record_single_unique : dataframe\n","        Records de los features que solo tienen un valor unico\n","    corr_matrix : dataframe\n","        Todas las correlaciones entre todos los features de la data\n","    record_collinear : dataframe\n","        Registra los pares de variables colineales con un coeficiente de correlación por encima del umbral\n","    feature_importances : dataframe\n","        Todas cuentan los features importantes de la gradient boosting machine\n","    record_zero_importance : dataframe\n","        Registra las características de importancia cero en los datos según el gbm\n","    record_low_importance : dataframe\n","        Registra las características de menor importancia que no son necesarias para alcanzar el umbral de importancia acumulativa según el GBM\n","\n","    Notas\n","    --------\n","    \n","        - Todas las 5 operaciones se pueden correr con el metodo `identify_all`.\n","        - Si usas feature importances, one-hot encoding se usa para las variables categoricas creando nuevas columnas\n","    \"\"\"\n","    \n","    def __init__(self, data, labels=None):    \n","        # Dataset y opcionalmente los training labels\n","        self.data = data\n","        self.labels = labels\n","        if labels is None:\n","            print('No labels provistos. Metodos de Feature importance no estaran disponibles.')\n","        self.base_features = list(data.columns)\n","        self.one_hot_features = None\n","        # Dataframes que guardan la informacion de los features a remover\n","        self.record_missing = None\n","        self.record_single_unique = None\n","        self.record_collinear = None\n","        self.record_zero_importance = None\n","        self.record_low_importance = None   \n","        self.missing_stats = None\n","        self.unique_stats = None\n","        self.corr_matrix = None\n","        self.feature_importances = None\n","        # Dictionary para realizar operaciones de remoción\n","        self.ops = {}\n","        self.one_hot_correlated = False\n","        \n","    def identify_missing(self, missing_threshold):\n","        \"\"\"Encontrar los features con una fraccion de missing values por encima del `missing_threshold`\"\"\"\n","        self.missing_threshold = missing_threshold\n","        # Calcular la fraccion de missing values en cada columna\n","        missing_series = self.data.isnull().sum() / self.data.shape[0]\n","        self.missing_stats = pd.DataFrame(missing_series).rename(columns = {'index': 'feature', 0: 'missing_fraction'})\n","        # Ordenar por el valor mas alto de valores missing en el top\n","        self.missing_stats = self.missing_stats.sort_values('missing_fraction', ascending = False)\n","        # Encontrar las columnas con porcentaje de missing arriba de cierto threshold\n","        record_missing = pd.DataFrame(missing_series[missing_series > missing_threshold]).reset_index().rename(columns = \n","                                                                                                               {'index': 'feature', \n","                                                                                                                0: 'missing_fraction'})\n","        to_drop = list(record_missing['feature'])\n","        self.record_missing = record_missing\n","        self.ops['missing'] = to_drop\n","        print('%d features con cantidad mayor a %0.2f en missing values.\\n' % (len(self.ops['missing']), self.missing_threshold))\n","    def identify_single_unique(self):\n","        \"\"\"Encontrar los features con solo un unico valor. NaNs no cuentan como valor unico. \"\"\"\n","        # Calcular los valores unicos en cada columna\n","        unique_counts = self.data.nunique()\n","        self.unique_stats = pd.DataFrame(unique_counts).rename(columns = {'index': 'feature', 0: 'nunique'})\n","        self.unique_stats = self.unique_stats.sort_values('nunique', ascending = True)        \n","        # Encontrar las columnas con solo un unico valor \n","        record_single_unique = pd.DataFrame(unique_counts[unique_counts == 1]).reset_index().rename(columns = {'index': 'feature', \n","                                                                                                                0: 'nunique'})\n","        to_drop = list(record_single_unique['feature'])\n","        self.record_single_unique = record_single_unique\n","        self.ops['single_unique'] = to_drop\n","        \n","        print('%d features con un valor unico .\\n' % len(self.ops['single_unique']))\n","    def identify_collinear(self, correlation_threshold, one_hot=False):\n","        \"\"\"\n","        Encuentra características colineales según el coeficiente de correlación entre características.\n","         Para cada par de características con un coeficiente de correlación mayor que \"correlation_threshold\",\n","         sólo uno de la pareja se identifica para eliminacion        \n","        Este codigo esta adaptado de: https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/\n","        Parametros\n","        --------\n","        correlation_threshold : float between 0 and 1\n","            Valor de la correlacion de  Pearson para identificat relaciones\n","        one_hot : boolean, default = False\n","            Cuando desee usar one-hot encode para los features antes de calcular los coef. de correlacion\n","        \"\"\"\n","        self.correlation_threshold = correlation_threshold\n","        self.one_hot_correlated = one_hot\n","         # Calcular la correlaciones entre cada columna\n","        if one_hot:\n","            # One hot encoding\n","            features = pd.get_dummies(self.data)\n","            self.one_hot_features = [column for column in features.columns if column not in self.base_features]\n","            # Agregar one hot encoded data a la data original\n","            self.data_all = pd.concat([features[self.one_hot_features], self.data], axis = 1)\n","            corr_matrix = pd.get_dummies(features).corr()\n","        else:\n","            corr_matrix = self.data.corr()\n","        self.corr_matrix = corr_matrix\n","    \n","        # Extraer el triangulo superior derecho de la matriz de correlacion\n","        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n","        # Seleccionar los features con correlacion por encima de un threshold\n","        # Se necesita usar valor absoluto \n","        to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n","        # Dataframe para almacenar las parejas correlacionadas\n","        record_collinear = pd.DataFrame(columns = ['drop_feature', 'corr_feature', 'corr_value'])\n","        # Iterar sobre las columnas para eliminar los records de parejas con features correlacionados\n","        for column in to_drop:\n","            #Encontrar los features correlacionados\n","            corr_features = list(upper.index[upper[column].abs() > correlation_threshold])\n","            #Encontrar los valores correlacionados\n","            corr_values = list(upper[column][upper[column].abs() > correlation_threshold])\n","            drop_features = [column for _ in range(len(corr_features))]   \n","            # guardar la info (usamos un temp df por ahora)\n","            temp_df = pd.DataFrame.from_dict({'drop_feature': drop_features,\n","                                             'corr_feature': corr_features,\n","                                             'corr_value': corr_values})\n","            # Agregar a un data frame\n","            record_collinear = record_collinear.append(temp_df, ignore_index = True)\n","        self.record_collinear = record_collinear\n","        self.ops['collinear'] = to_drop\n","        \n","        print('%d features con una mangitud de correlacion mayor que %0.2f.\\n' % (len(self.ops['collinear']), self.correlation_threshold))\n","\n","    def identify_zero_importance(self, task, eval_metric=None, \n","                                 n_iterations=10, early_stopping = True):\n","        \"\"\"\n","        \n","        Identificar los features con 0 importancia acorde con la gradient boosting machine.\n","        El gbm puede ser entrenado con un parametro llamado early stopping usando un test de validacion para prevenir el overfitting. \n","        Las feature importances se promedian sobre todas las `n_iterations` para reducir la varianza. \n","        Uso de la implementacion LightGBM, pueden leer: (http://lightgbm.readthedocs.io/en/latest/index.html)\n","        Parametros\n","        -------\n","        eval_metric : string\n","            Metrica de evaluacion usada para el gradient boosting con early stopping. Debe proveerse `early_stopping` is True\n","        task : string\n","            La machine learning task, puede ser 'classification' o 'regression'\n","        n_iterations : int, default = 10\n","            Numero de iteraciones para la gradient boosting machine\n","        early_stopping : boolean, default = True\n","            Cuando o no usar early stopping con validation set cuendo se entrene\n","        Notes\n","        --------\n","        - Features se les aplica one-hot encoded para manipular variables categoricas antes de training.\n","        - El gbm no es optimizado para una tarea particular y puede ser necesario algun hyperparameter tuning\n","        - Las feature importances, incluyendo los importance features 0 ,pueden cambiar en diferentes corridas pero no mucho\n","        \"\"\"\n","        if early_stopping and eval_metric is None:\n","            raise ValueError(\"\"\"eval metric debe proveerse con early stopping. Algunos ejemplos \"auc\" para classification o\n","                             \"l2\"para regression.\"\"\")\n","            \n","        if self.labels is None:\n","            raise ValueError(\"No training labels se proveen.\")\n","        \n","        # One hot encoding\n","        features = pd.get_dummies(self.data)\n","        self.one_hot_features = [column for column in features.columns if column not in self.base_features]\n","        # Agregar la data one hot encoded a la data original\n","        self.data_all = pd.concat([features[self.one_hot_features], self.data], axis = 1)\n","        # Extraer los feature names\n","        feature_names = list(features.columns)\n","        # Convertir a np array\n","        features = np.array(features)\n","        labels = np.array(self.labels).reshape((-1, ))\n","        # Empty array para feature importances\n","        feature_importance_values = np.zeros(len(feature_names))\n","        print('Entrenar el Gradient Boosting Model\\n')\n","        # Iterate sobre cada fold\n","        for _ in range(n_iterations):\n","            if task == 'classification':\n","                model = lgb.LGBMClassifier(n_estimators=1000, learning_rate = 0.05, verbose = -1)\n","            elif task == 'regression':\n","                model = lgb.LGBMRegressor(n_estimators=1000, learning_rate = 0.05, verbose = -1)\n","            else:\n","                raise ValueError('La tarea deber ser \"classification\" o \"regression\"')\n","            # Si en entrenamiento se usa early stopping se necesita validation set\n","            if early_stopping:\n","                train_features, valid_features, train_labels, valid_labels = train_test_split(features, labels, test_size = 0.15, stratify=labels)\n","                # Entrenar el modelo con early stopping\n","                model.fit(train_features, train_labels, eval_metric = eval_metric,\n","                          eval_set = [(valid_features, valid_labels)],\n","                          early_stopping_rounds = 100, verbose = -1)\n","                # Limpiar memory\n","                gc.enable()\n","                del train_features, train_labels, valid_features, valid_labels\n","                gc.collect()\n","            else:\n","                model.fit(features, labels)\n","            # Guardar los feature importances\n","            feature_importance_values += model.feature_importances_ / n_iterations\n","        feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n","        # Ordenar los features de acuerdo a su importancia\n","        feature_importances = feature_importances.sort_values('importance', ascending = False).reset_index(drop = True)\n","        # Normalizar las feature importances para que sumados den 1\n","        feature_importances['normalized_importance'] = feature_importances['importance'] / feature_importances['importance'].sum()\n","        feature_importances['cumulative_importance'] = np.cumsum(feature_importances['normalized_importance'])\n","        # Extraer los features con 0 importancia\n","        record_zero_importance = feature_importances[feature_importances['importance'] == 0.0]\n","        to_drop = list(record_zero_importance['feature'])\n","        self.feature_importances = feature_importances\n","        self.record_zero_importance = record_zero_importance\n","        self.ops['zero_importance'] = to_drop\n","        print('\\n%d features con cero importancia despues de one-hot encoding.\\n' % len(self.ops['zero_importance']))\n","    \n","    def identify_low_importance(self, cumulative_importance):\n","        \"\"\"\n","        Encontrar los fetures con mas bajo importance usando `cumulative_importance` \n","        del total de importancia obtenido del gradient boosting machine. Por ejemplo si la importancia acumulada\n","        es 0.95, esto retendra solo los features mas importantes necesarios para alcanzar\n","        95% del total de feature importance. \n","        Parametros\n","        --------\n","        cumulative_importance : float between 0 and 1\n","            Fraccion de la importancia acumulada a tener en cuenta\n","        \"\"\"\n","        self.cumulative_importance = cumulative_importance\n","        # Las feature importances se deben calcular primero antes de correr \n","        if self.feature_importances is None:\n","            raise NotImplementedError(\"\"\"Feature importances aun no se han determinado. \n","                                         Llamar al metodo `identify_zero_importance` primero.\"\"\")\n","        #Asegurarse que los features mas importantes estan en el top\n","        self.feature_importances = self.feature_importances.sort_values('cumulative_importance')\n","        # Identificar los features que no son necesarios para alcanzar la cumulative_importance\n","        record_low_importance = self.feature_importances[self.feature_importances['cumulative_importance'] > cumulative_importance]\n","        to_drop = list(record_low_importance['feature'])\n","        self.record_low_importance = record_low_importance\n","        self.ops['low_importance'] = to_drop\n","        print('%d features requeridos para el cumulative importance de %0.2f luego one hot encoding.' % (len(self.feature_importances) -\n","                                                                            len(self.record_low_importance), self.cumulative_importance))\n","        print('%d features que no contribuyen a la cumulative importance de %0.2f.\\n' % (len(self.ops['low_importance']),\n","                                                                                               self.cumulative_importance))\n","    def identify_all(self, selection_params):\n","        \"\"\"\n","        Usar los 5 metodos para identificar los nombres de features a remover\n","        Parametros\n","        --------\n","        selection_params : dict\n","           Parametros a usar en los 5 feature selection methhods.\n","           Params puede contener los keys ['missing_threshold', 'correlation_threshold', 'eval_metric', 'task', 'cumulative_importance']\n","        \n","        \"\"\"\n","        # Check de los parametros requeridos\n","        for param in ['missing_threshold', 'correlation_threshold', 'eval_metric', 'task', 'cumulative_importance']:\n","            if param not in selection_params.keys():\n","                raise ValueError('%s se requiere un parametro para este metodo.' % param)\n","        # Implementar cada uno de los 5 metodos\n","        self.identify_missing(selection_params['missing_threshold'])\n","        self.identify_single_unique()\n","        self.identify_collinear(selection_params['correlation_threshold'])\n","        self.identify_zero_importance(task = selection_params['task'], eval_metric = selection_params['eval_metric'])\n","        self.identify_low_importance(selection_params['cumulative_importance'])\n","        # Encontrar el numero de features a eliminar \n","        self.all_identified = set(list(chain(*list(self.ops.values()))))\n","        self.n_identified = len(self.all_identified)  \n","        print('%d total de features en %d identificados para remocion despues de one-hot encoding.\\n' % (self.n_identified, \n","                                                                                                  self.data_all.shape[1]))\n","    def check_removal(self, keep_one_hot=True):\n","        \"\"\"Check los features identificados antes de remover. Devuelve una lista de features unicos identificados.\"\"\"\n","        self.all_identified = set(list(chain(*list(self.ops.values()))))\n","        print('Total de %d features identificados para remocion' % len(self.all_identified))\n","        if not keep_one_hot:\n","            if self.one_hot_features is None:\n","                print('Data no se le ha aplicado one-hot encoded')\n","            else:\n","                one_hot_to_remove = [x for x in self.one_hot_features if x not in self.all_identified]\n","                print('%d one-hot features adicionales que pueden removerse' % len(one_hot_to_remove))\n","        return list(self.all_identified)\n","        \n","    def remove(self, methods, keep_one_hot = True):\n","        \"\"\"\n","        Elimine las características de los datos de acuerdo con los métodos especificados.\n","        \n","         Parámetros\n","        --------\n","            methods : 'all' or list of methods\n","                If methods == 'all', todos los metodos seran usados. En otro caso solo la seleccion que provea.\n","                Opciones: ['missing', 'single_unique', 'collinear', 'zero_importance', 'low_importance']\n","            keep_one_hot : boolean, default = True\n","                Cuando o no mantener los features de one-hot encoded \n","        Return\n","        --------\n","            data : dataframe\n","                Dataframe con los features identificados removidos \n","        Notes \n","        --------\n","            - Si se utilizan características importantes, las columnas codificadas en un solo uso se agregarán a los datos (y luego se pueden eliminar)\n","             - ¡Compruebe las funciones que eliminarán antes de transformar los datos!\n","        \"\"\"\n","        features_to_drop = []\n","        if methods == 'all':\n","            # Necesita el uso de  one-hot encoded data \n","            data = self.data_all\n","            print('{} metodos que se han utilizado\\n'.format(list(self.ops.keys())))\n","            # Encontrar los features unicos a remover\n","            features_to_drop = set(list(chain(*list(self.ops.values()))))\n","        else:\n","            # Necesita el uso de  one-hot encoded \n","            if 'zero_importance' in methods or 'low_importance' in methods or self.one_hot_correlated:\n","                data = self.data_all\n","            else:\n","                data = self.data\n","            # Iterar sobre los metodos especificos \n","            for method in methods:\n","                # Check del metodo que se ha usad\n","                if method not in self.ops.keys():\n","                    raise NotImplementedError('%s method no ha sido implementado ' % method)\n","                # Agregar a la lista los features identificados a remover\n","                else:\n","                    features_to_drop.append(self.ops[method])\n","            # Encontrar los features unicos a remover \n","            features_to_drop = set(list(chain(*features_to_drop)))\n","        features_to_drop = list(features_to_drop)\n","        if not keep_one_hot:\n","            if self.one_hot_features is None:\n","                print('Data no ha sido one-hot encoded')\n","            else: \n","                features_to_drop = list(set(features_to_drop) | set(self.one_hot_features))\n","        # Remover los features y devolver la data\n","        data = data.drop(columns = features_to_drop)\n","        self.removed_features = features_to_drop\n","        if not keep_one_hot:\n","        \tprint('Removidos %d features incluyendo one-hot features.' % len(features_to_drop))\n","        else:\n","        \tprint('Removidos %d features.' % len(features_to_drop))\n","        return data\n","    \n","    def plot_missing(self):\n","        \"\"\"Histograma de la fraccion de missing en cada features\"\"\"\n","        if self.record_missing is None:\n","            raise NotImplementedError(\"Missing values no han sido calculados aun. Run `identify_missing`\")\n","        self.reset_plot()\n","        # Histograma de missing values\n","        plt.style.use('seaborn-white')\n","        plt.figure(figsize = (7, 5))\n","        plt.hist(self.missing_stats['missing_fraction'], bins = np.linspace(0, 1, 11), edgecolor = 'k', color = 'red', linewidth = 1.5)\n","        plt.xticks(np.linspace(0, 1, 11));\n","        plt.xlabel('Missing Fraction', size = 14); plt.ylabel('Count of Features', size = 14); \n","        plt.title(\"Fraction of Missing Values Histogram\", size = 16);\n","    def plot_unique(self):\n","        \"\"\"Histograma de valores unicos para cada feature\"\"\"\n","        if self.record_single_unique is None:\n","            raise NotImplementedError('Unique values no han sido calculados. Run `identify_single_unique`')\n","        self.reset_plot()\n","        # Histograma de valores unicos\n","        self.unique_stats.plot.hist(edgecolor = 'k', figsize = (7, 5))\n","        plt.ylabel('Frequency', size = 14); plt.xlabel('Unique Values', size = 14); \n","        plt.title('Number of Unique Values Histogram', size = 16);\n","    def plot_collinear(self, plot_all = False):\n","        \"\"\"\n","       Mapa de calor de los valores de correlación. Si plot_all = True traza todas las correlaciones de lo contrario\n","         traza solo aquellas características que tienen una correlación por encima del umbral \n","        Notas\n","        --------\n","            - No todas las correlaciones trazadas están por encima del umbral porque en esta grafica\n","             todas las variables que han sido identificadas por tener incluso una correlación por encima del umbral\n","             - Las características del eje x son las que se eliminarán. Las características en el eje y\n","             son las características correlacionadas con las del eje x\n","        Codigo adaptado de https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n","        \"\"\"\n","        if self.record_collinear is None:\n","            raise NotImplementedError('Collinear features have no se han identificado. Run `identify_collinear`.')\n","        if plot_all:\n","        \tcorr_matrix_plot = self.corr_matrix\n","        \ttitle = 'All Correlations'\n","        else:\n","\t        # Identificar correlaciones por encima de un threshold\n","\t        # columnas (x-axis) son los features a eliminar y rows (y_axis) son las parejas correlacionadas\n","\t        corr_matrix_plot = self.corr_matrix.loc[list(set(self.record_collinear['corr_feature'])), \n","\t                                                list(set(self.record_collinear['drop_feature']))]\n","\t        title = \"Correlations Above Threshold\"  \n","        f, ax = plt.subplots(figsize=(10, 8))\n","        # Diverging colormap\n","        cmap = sns.diverging_palette(220, 10, as_cmap=True)\n","        # Hacer el heatmap con el color bar\n","        sns.heatmap(corr_matrix_plot, cmap=cmap, center=0,\n","                    linewidths=.25, cbar_kws={\"shrink\": 0.6})\n","        # Ajustar ylabels \n","        ax.set_yticks([x + 0.5 for x in list(range(corr_matrix_plot.shape[0]))])\n","        ax.set_yticklabels(list(corr_matrix_plot.index), size = int(160 / corr_matrix_plot.shape[0]));\n","        # Ajustar xlabels \n","        ax.set_xticks([x + 0.5 for x in list(range(corr_matrix_plot.shape[1]))])\n","        ax.set_xticklabels(list(corr_matrix_plot.columns), size = int(160 / corr_matrix_plot.shape[1]));\n","        plt.title(title, size = 14)\n","    def plot_feature_importances(self, plot_n = 15, threshold = None):\n","        \"\"\"\n","        Plots `plot_n` los features mas importantes teniendo enc uenta la cumulative importance.\n","        Si `threshold` se provee, imprime el numero de features necesarios para alcanzar el `threshold` de cumulative importance.\n","        Parametros\n","        --------\n","        plot_n : int, default = 15\n","            Número de características más importantes para graficar. El valor predeterminado es 15 o el número máximo de funciones, lo que sea menor.\n","        threshold : float, between 0 and 1 default = None\n","            Umbral para imprimir información sobre importancias acumulativas\n","        \"\"\"\n","        if self.record_zero_importance is None:\n","            raise NotImplementedError('Feature importances no han sido determinadas. Run `idenfity_zero_importance`')\n","        # Necesita ajustar el número de características si es mayor que las características en los datos\n","        if plot_n > self.feature_importances.shape[0]:\n","            plot_n = self.feature_importances.shape[0] - 1\n","        self.reset_plot()\n","        # Hacer un gráfico de barras horizontales de la importancia de las características\n","        plt.figure(figsize = (10, 6))\n","        ax = plt.subplot()\n","        # Es necesario invertir el índice para trazar los más importantes en la parte superior.\n","         # Podría haber un método más eficiente para lograr esto\n","        ax.barh(list(reversed(list(self.feature_importances.index[:plot_n]))), \n","                self.feature_importances['normalized_importance'][:plot_n], \n","                align = 'center', edgecolor = 'k')\n","        # Set los yticks t labels\n","        ax.set_yticks(list(reversed(list(self.feature_importances.index[:plot_n]))))\n","        ax.set_yticklabels(self.feature_importances['feature'][:plot_n], size = 12)\n","        # Plot\n","        plt.xlabel('Normalized Importance', size = 16); plt.title('Feature Importances', size = 18)\n","        plt.show()\n","        # Cumulative importance plot\n","        plt.figure(figsize = (6, 4))\n","        plt.plot(list(range(1, len(self.feature_importances) + 1)), self.feature_importances['cumulative_importance'], 'r-')\n","        plt.xlabel('Number of Features', size = 14); plt.ylabel('Cumulative Importance', size = 14); \n","        plt.title('Cumulative Feature Importance', size = 16);\n","        if threshold:\n","            # Índice de la cantidad mínima de características necesarias para el umbral de importancia acumulativa\n","             # np.where devuelve el índice, por lo que es necesario agregar 1 para tener el número correcto\n","            importance_index = np.min(np.where(self.feature_importances['cumulative_importance'] > threshold))\n","            plt.vlines(x = importance_index + 1, ymin = 0, ymax = 1, linestyles='--', colors = 'blue')\n","            plt.show();\n","            print('%d features requidos para un %0.2f de cumulative importance' % (importance_index + 1, threshold))\n","    def reset_plot(self):\n","        plt.rcParams = plt.rcParamsDefault"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0 features con cantidad mayor a 0.70 en missing values.\n","\n","0 features con un valor unico .\n","\n","1 features con una mangitud de correlacion mayor que 0.91.\n","\n","Entrenar el Gradient Boosting Model\n","\n","\n","1 features con cero importancia despues de one-hot encoding.\n","\n","18 features requeridos para el cumulative importance de 0.99 luego one hot encoding.\n","3 features que no contribuyen a la cumulative importance de 0.99.\n","\n","3 total de features en 21 identificados para remocion despues de one-hot encoding.\n","\n"]}],"source":["fs = FeatureSelector(data = x, labels = y)\n","\n","fs.identify_all(selection_params = {'missing_threshold': 0.7, 'correlation_threshold': 0.91, \n","                                    'task': 'classification', 'eval_metric': 'auc', \n","                                     'cumulative_importance': 0.99})"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Se remueven las columnas determinadas por el Feature selection"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['missing', 'single_unique', 'collinear', 'zero_importance', 'low_importance'] metodos que se han utilizado\n","\n","Removidos 3 features.\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>height(cm)</th>\n","      <th>weight(kg)</th>\n","      <th>waist(cm)</th>\n","      <th>eyesight(left)</th>\n","      <th>eyesight(right)</th>\n","      <th>fasting blood sugar</th>\n","      <th>Cholesterol</th>\n","      <th>triglyceride</th>\n","      <th>HDL</th>\n","      <th>LDL</th>\n","      <th>hemoglobin</th>\n","      <th>serum creatinine</th>\n","      <th>AST</th>\n","      <th>ALT</th>\n","      <th>Gtp</th>\n","      <th>IMC</th>\n","      <th>TAM</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.297269</td>\n","      <td>-1.050222</td>\n","      <td>-0.384530</td>\n","      <td>-0.040085</td>\n","      <td>0.611271</td>\n","      <td>0.111054</td>\n","      <td>-0.125467</td>\n","      <td>0.541670</td>\n","      <td>-0.527427</td>\n","      <td>1.103291</td>\n","      <td>0.407949</td>\n","      <td>-1.126847</td>\n","      <td>-0.940852</td>\n","      <td>-0.864862</td>\n","      <td>-0.211448</td>\n","      <td>0.054721</td>\n","      <td>0.322204</td>\n","      <td>-0.470663</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.297269</td>\n","      <td>-0.528799</td>\n","      <td>-0.384530</td>\n","      <td>-0.072702</td>\n","      <td>-0.489408</td>\n","      <td>-1.155756</td>\n","      <td>1.690480</td>\n","      <td>-0.079764</td>\n","      <td>0.103300</td>\n","      <td>-1.132692</td>\n","      <td>0.434100</td>\n","      <td>-1.233483</td>\n","      <td>-1.567788</td>\n","      <td>-0.140581</td>\n","      <td>-0.211448</td>\n","      <td>-0.627463</td>\n","      <td>-0.124719</td>\n","      <td>-0.261007</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.905592</td>\n","      <td>0.564979</td>\n","      <td>-0.384530</td>\n","      <td>-0.182056</td>\n","      <td>-0.489408</td>\n","      <td>-0.475679</td>\n","      <td>-0.561403</td>\n","      <td>1.221340</td>\n","      <td>0.943687</td>\n","      <td>-0.033459</td>\n","      <td>1.022712</td>\n","      <td>0.745089</td>\n","      <td>0.639395</td>\n","      <td>-0.298108</td>\n","      <td>-0.564867</td>\n","      <td>-0.275446</td>\n","      <td>-1.000805</td>\n","      <td>1.204194</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-0.297269</td>\n","      <td>0.009596</td>\n","      <td>0.413851</td>\n","      <td>0.666754</td>\n","      <td>1.275418</td>\n","      <td>1.289465</td>\n","      <td>0.030444</td>\n","      <td>2.996936</td>\n","      <td>1.542069</td>\n","      <td>-0.849996</td>\n","      <td>2.519575</td>\n","      <td>-0.039193</td>\n","      <td>0.639395</td>\n","      <td>-0.657974</td>\n","      <td>0.378473</td>\n","      <td>-0.627463</td>\n","      <td>0.524383</td>\n","      <td>-1.812592</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.297269</td>\n","      <td>-1.050222</td>\n","      <td>-0.384530</td>\n","      <td>0.459942</td>\n","      <td>0.097565</td>\n","      <td>0.111054</td>\n","      <td>-1.557496</td>\n","      <td>-0.306532</td>\n","      <td>-0.720712</td>\n","      <td>0.449791</td>\n","      <td>-0.118938</td>\n","      <td>-1.337390</td>\n","      <td>-1.567788</td>\n","      <td>-1.347831</td>\n","      <td>-0.854762</td>\n","      <td>-0.275446</td>\n","      <td>0.322204</td>\n","      <td>-0.085278</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>55686</th>\n","      <td>1.277257</td>\n","      <td>-1.554690</td>\n","      <td>-1.332362</td>\n","      <td>-0.744104</td>\n","      <td>0.097565</td>\n","      <td>0.624829</td>\n","      <td>-0.965864</td>\n","      <td>-0.451334</td>\n","      <td>-1.354699</td>\n","      <td>-0.260813</td>\n","      <td>0.138123</td>\n","      <td>-1.285776</td>\n","      <td>-0.368763</td>\n","      <td>0.377833</td>\n","      <td>-0.015611</td>\n","      <td>-1.110260</td>\n","      <td>-0.505758</td>\n","      <td>-1.672152</td>\n","    </tr>\n","    <tr>\n","      <th>55688</th>\n","      <td>0.120440</td>\n","      <td>-0.528799</td>\n","      <td>-1.332362</td>\n","      <td>-1.333799</td>\n","      <td>0.611271</td>\n","      <td>0.624829</td>\n","      <td>-0.561403</td>\n","      <td>-0.839770</td>\n","      <td>-0.852907</td>\n","      <td>1.103291</td>\n","      <td>-1.029945</td>\n","      <td>-0.490168</td>\n","      <td>0.155945</td>\n","      <td>-0.469885</td>\n","      <td>-1.206312</td>\n","      <td>-1.110260</td>\n","      <td>-1.453434</td>\n","      <td>-1.672152</td>\n","    </tr>\n","    <tr>\n","      <th>55689</th>\n","      <td>0.905592</td>\n","      <td>-0.528799</td>\n","      <td>-1.332362</td>\n","      <td>-1.516599</td>\n","      <td>0.097565</td>\n","      <td>0.624829</td>\n","      <td>-0.657459</td>\n","      <td>-1.088115</td>\n","      <td>-0.645787</td>\n","      <td>1.417086</td>\n","      <td>-1.656430</td>\n","      <td>-1.388329</td>\n","      <td>-2.258780</td>\n","      <td>-1.093576</td>\n","      <td>-1.412794</td>\n","      <td>-1.430352</td>\n","      <td>-1.453434</td>\n","      <td>-0.320390</td>\n","    </tr>\n","    <tr>\n","      <th>55690</th>\n","      <td>1.277257</td>\n","      <td>0.009596</td>\n","      <td>-0.384530</td>\n","      <td>-0.403754</td>\n","      <td>-0.489408</td>\n","      <td>0.111054</td>\n","      <td>0.739741</td>\n","      <td>0.410167</td>\n","      <td>-0.597508</td>\n","      <td>-0.586445</td>\n","      <td>0.905773</td>\n","      <td>-0.236945</td>\n","      <td>-0.940852</td>\n","      <td>-0.469885</td>\n","      <td>-0.211448</td>\n","      <td>-0.627463</td>\n","      <td>-0.565665</td>\n","      <td>0.690330</td>\n","    </tr>\n","    <tr>\n","      <th>55691</th>\n","      <td>0.905592</td>\n","      <td>-0.528799</td>\n","      <td>0.030394</td>\n","      <td>0.355234</td>\n","      <td>-0.185649</td>\n","      <td>-0.802468</td>\n","      <td>-1.308076</td>\n","      <td>0.489296</td>\n","      <td>0.491671</td>\n","      <td>-2.004351</td>\n","      <td>0.999543</td>\n","      <td>0.165388</td>\n","      <td>-0.368763</td>\n","      <td>0.377833</td>\n","      <td>0.567831</td>\n","      <td>0.644218</td>\n","      <td>0.437326</td>\n","      <td>0.171758</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>50122 rows × 18 columns</p>\n","</div>"],"text/plain":["            age  height(cm)  weight(kg)  waist(cm)  eyesight(left)  \\\n","0     -0.297269   -1.050222   -0.384530  -0.040085        0.611271   \n","1     -0.297269   -0.528799   -0.384530  -0.072702       -0.489408   \n","2      0.905592    0.564979   -0.384530  -0.182056       -0.489408   \n","3     -0.297269    0.009596    0.413851   0.666754        1.275418   \n","4     -0.297269   -1.050222   -0.384530   0.459942        0.097565   \n","...         ...         ...         ...        ...             ...   \n","55686  1.277257   -1.554690   -1.332362  -0.744104        0.097565   \n","55688  0.120440   -0.528799   -1.332362  -1.333799        0.611271   \n","55689  0.905592   -0.528799   -1.332362  -1.516599        0.097565   \n","55690  1.277257    0.009596   -0.384530  -0.403754       -0.489408   \n","55691  0.905592   -0.528799    0.030394   0.355234       -0.185649   \n","\n","       eyesight(right)  fasting blood sugar  Cholesterol  triglyceride  \\\n","0             0.111054            -0.125467     0.541670     -0.527427   \n","1            -1.155756             1.690480    -0.079764      0.103300   \n","2            -0.475679            -0.561403     1.221340      0.943687   \n","3             1.289465             0.030444     2.996936      1.542069   \n","4             0.111054            -1.557496    -0.306532     -0.720712   \n","...                ...                  ...          ...           ...   \n","55686         0.624829            -0.965864    -0.451334     -1.354699   \n","55688         0.624829            -0.561403    -0.839770     -0.852907   \n","55689         0.624829            -0.657459    -1.088115     -0.645787   \n","55690         0.111054             0.739741     0.410167     -0.597508   \n","55691        -0.802468            -1.308076     0.489296      0.491671   \n","\n","            HDL       LDL  hemoglobin  serum creatinine       AST       ALT  \\\n","0      1.103291  0.407949   -1.126847         -0.940852 -0.864862 -0.211448   \n","1     -1.132692  0.434100   -1.233483         -1.567788 -0.140581 -0.211448   \n","2     -0.033459  1.022712    0.745089          0.639395 -0.298108 -0.564867   \n","3     -0.849996  2.519575   -0.039193          0.639395 -0.657974  0.378473   \n","4      0.449791 -0.118938   -1.337390         -1.567788 -1.347831 -0.854762   \n","...         ...       ...         ...               ...       ...       ...   \n","55686 -0.260813  0.138123   -1.285776         -0.368763  0.377833 -0.015611   \n","55688  1.103291 -1.029945   -0.490168          0.155945 -0.469885 -1.206312   \n","55689  1.417086 -1.656430   -1.388329         -2.258780 -1.093576 -1.412794   \n","55690 -0.586445  0.905773   -0.236945         -0.940852 -0.469885 -0.211448   \n","55691 -2.004351  0.999543    0.165388         -0.368763  0.377833  0.567831   \n","\n","            Gtp       IMC       TAM  \n","0      0.054721  0.322204 -0.470663  \n","1     -0.627463 -0.124719 -0.261007  \n","2     -0.275446 -1.000805  1.204194  \n","3     -0.627463  0.524383 -1.812592  \n","4     -0.275446  0.322204 -0.085278  \n","...         ...       ...       ...  \n","55686 -1.110260 -0.505758 -1.672152  \n","55688 -1.110260 -1.453434 -1.672152  \n","55689 -1.430352 -1.453434 -0.320390  \n","55690 -0.627463 -0.565665  0.690330  \n","55691  0.644218  0.437326  0.171758  \n","\n","[50122 rows x 18 columns]"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["x = fs.remove(methods = 'all', keep_one_hot = True)\n","x"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Confeccion de modelo"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: xgboost in /usr/local/python/3.10.4/lib/python3.10/site-packages (1.7.5)\n","Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.10/site-packages (from xgboost) (1.9.3)\n","Requirement already satisfied: numpy in /usr/local/python/3.10.4/lib/python3.10/site-packages (from xgboost) (1.24.3)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install xgboost"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.7636907730673317\n","Accuracy: 0.7546134663341646\n","Accuracy: 0.7573822825219473\n","Accuracy: 0.7589784517158819\n","Accuracy: 0.8272146847565842\n"]}],"source":["import xgboost as xgb\n","from sklearn.model_selection import StratifiedKFold\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import accuracy_score\n","\n","\n","\n","# Restablecer los índices de x\n","x = x.reset_index(drop=True)\n","\n","# Crear el objeto StratifiedKFold\n","kfold = StratifiedKFold(n_splits=5)\n","\n","# Realizar la validación cruzada\n","for train_index, test_index in kfold.split(x, y):\n","    X_train, X_test = x.iloc[train_index], x.iloc[test_index]\n","    y_train, y_test = y.iloc[train_index], y.iloc[test_index]  # Use iloc instead of direct indexing\n","\n","    # Create and train the XGBoost model\n","    model = xgb.XGBClassifier()\n","    model.fit(X_train, y_train)\n","    # Perform predictions and evaluate the model\n","    predictions = model.predict(X_test)\n","    accuracy = accuracy_score(y_test, predictions)\n","    print(f\"Accuracy: {accuracy}\")\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Conclusion preliminar"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Se eligio como modelo para realizar la Cross_Validation XGBoost. La similiitud entre los resultados demuestra una buen performance sin hacer overfitting."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"}}},"nbformat":4,"nbformat_minor":2}
