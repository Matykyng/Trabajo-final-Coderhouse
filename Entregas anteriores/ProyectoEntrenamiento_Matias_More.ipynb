{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Proyecto Final\n","by Matías Moré"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["#Librerias\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","sns.set_palette([\"#024059\", \"#66D9CD\", \"#D9923B\", \"#BF726B\", \"#401F1F\"])\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Dataset Fumadores"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Este conjunto de datos consiste en datos biologicos de distintos individuos, recolectados con el objetivo de analizar los efectos y las señales del habito tabaquico. A travez de distintas caracteristicas de estudios de laboratorio como el estado sanguineo, urinario, hepatico, higienico y su relacion con el tabaquismo, se buscara encontrar patrones que determinen biologicamente los parametros a tener en cuenta para identificar a individuos fumadores. Teniendo en cuenta estos datos se realizara una comparacion de estas variables a fin de determinar su utilidad y trascendencia para lograr este fin.\n","\n","Origen del dataset: https://www.kaggle.com/datasets/kukuroo3/body-signal-of-smoking?resource=download"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Definición de objetivo"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["En este trabajo se intenta desarrollar, a partir de los datos proporcionados, un modelo que permita predecir de la forma mas eficiente los individuos que sean fumadores."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Contexto comercial"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Se intenta elaborar una predicción a patir de las historias clínicas, para informar a los profesionales sobre si deben considerar la posibilidad de que sus pacientes sean fumadores y tomarlo en cuenta a la hora de elaborar tratamientos o indicar medicamentos."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Problema comercial"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["¿Es posible, con los datos de los pacientes en nuestra base de datos de historia clinica, encontrar patrones que identifiquen hábitos tabáquicos? ¿En qué grupos de pacientes  debemos incursionar más para la prevención de tabaquismo?"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Contexto analítico"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Se toman 55692 pacientes de la base de datos de historia clinica, a quienes se les identifican los estudios básicos completos y quienes cuentan con la verificacion profesional de si son fumadores o no. Estos datos se pretenden usar para encausar un modelo predictivo de aprendizaje supervisado que nos permita identificar los futuros casos de pacientes a quienes no se les determinó aún si son fumadores o no. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Code Acquisition"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>gender</th>\n","      <th>age</th>\n","      <th>height(cm)</th>\n","      <th>weight(kg)</th>\n","      <th>waist(cm)</th>\n","      <th>eyesight(left)</th>\n","      <th>eyesight(right)</th>\n","      <th>hearing(left)</th>\n","      <th>hearing(right)</th>\n","      <th>...</th>\n","      <th>hemoglobin</th>\n","      <th>Urine protein</th>\n","      <th>serum creatinine</th>\n","      <th>AST</th>\n","      <th>ALT</th>\n","      <th>Gtp</th>\n","      <th>oral</th>\n","      <th>dental caries</th>\n","      <th>tartar</th>\n","      <th>smoking</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>F</td>\n","      <td>40</td>\n","      <td>155</td>\n","      <td>60</td>\n","      <td>81.3</td>\n","      <td>1.2</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>12.9</td>\n","      <td>1.0</td>\n","      <td>0.7</td>\n","      <td>18.0</td>\n","      <td>19.0</td>\n","      <td>27.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>F</td>\n","      <td>40</td>\n","      <td>160</td>\n","      <td>60</td>\n","      <td>81.0</td>\n","      <td>0.8</td>\n","      <td>0.6</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>12.7</td>\n","      <td>1.0</td>\n","      <td>0.6</td>\n","      <td>22.0</td>\n","      <td>19.0</td>\n","      <td>18.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>M</td>\n","      <td>55</td>\n","      <td>170</td>\n","      <td>60</td>\n","      <td>80.0</td>\n","      <td>0.8</td>\n","      <td>0.8</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>15.8</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>21.0</td>\n","      <td>16.0</td>\n","      <td>22.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","      <td>N</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>M</td>\n","      <td>40</td>\n","      <td>165</td>\n","      <td>70</td>\n","      <td>88.0</td>\n","      <td>1.5</td>\n","      <td>1.5</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>14.7</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>19.0</td>\n","      <td>26.0</td>\n","      <td>18.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>F</td>\n","      <td>40</td>\n","      <td>155</td>\n","      <td>60</td>\n","      <td>86.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>12.5</td>\n","      <td>1.0</td>\n","      <td>0.6</td>\n","      <td>16.0</td>\n","      <td>14.0</td>\n","      <td>22.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","      <td>N</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>55687</th>\n","      <td>55676</td>\n","      <td>F</td>\n","      <td>40</td>\n","      <td>170</td>\n","      <td>65</td>\n","      <td>75.0</td>\n","      <td>0.9</td>\n","      <td>0.9</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>12.3</td>\n","      <td>1.0</td>\n","      <td>0.6</td>\n","      <td>14.0</td>\n","      <td>7.0</td>\n","      <td>10.0</td>\n","      <td>Y</td>\n","      <td>1</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>55688</th>\n","      <td>55681</td>\n","      <td>F</td>\n","      <td>45</td>\n","      <td>160</td>\n","      <td>50</td>\n","      <td>70.0</td>\n","      <td>1.2</td>\n","      <td>1.2</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>14.0</td>\n","      <td>1.0</td>\n","      <td>0.9</td>\n","      <td>20.0</td>\n","      <td>12.0</td>\n","      <td>14.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>55689</th>\n","      <td>55683</td>\n","      <td>F</td>\n","      <td>55</td>\n","      <td>160</td>\n","      <td>50</td>\n","      <td>68.5</td>\n","      <td>1.0</td>\n","      <td>1.2</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>12.4</td>\n","      <td>1.0</td>\n","      <td>0.5</td>\n","      <td>17.0</td>\n","      <td>11.0</td>\n","      <td>12.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","      <td>N</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>55690</th>\n","      <td>55684</td>\n","      <td>M</td>\n","      <td>60</td>\n","      <td>165</td>\n","      <td>60</td>\n","      <td>78.0</td>\n","      <td>0.8</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>14.4</td>\n","      <td>1.0</td>\n","      <td>0.7</td>\n","      <td>20.0</td>\n","      <td>19.0</td>\n","      <td>18.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","      <td>N</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>55691</th>\n","      <td>55691</td>\n","      <td>M</td>\n","      <td>55</td>\n","      <td>160</td>\n","      <td>65</td>\n","      <td>85.0</td>\n","      <td>0.9</td>\n","      <td>0.7</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>15.0</td>\n","      <td>1.0</td>\n","      <td>0.8</td>\n","      <td>26.0</td>\n","      <td>29.0</td>\n","      <td>41.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","      <td>Y</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>55692 rows × 27 columns</p>\n","</div>"],"text/plain":["          ID gender  age  height(cm)  weight(kg)  waist(cm)  eyesight(left)  \\\n","0          0      F   40         155          60       81.3             1.2   \n","1          1      F   40         160          60       81.0             0.8   \n","2          2      M   55         170          60       80.0             0.8   \n","3          3      M   40         165          70       88.0             1.5   \n","4          4      F   40         155          60       86.0             1.0   \n","...      ...    ...  ...         ...         ...        ...             ...   \n","55687  55676      F   40         170          65       75.0             0.9   \n","55688  55681      F   45         160          50       70.0             1.2   \n","55689  55683      F   55         160          50       68.5             1.0   \n","55690  55684      M   60         165          60       78.0             0.8   \n","55691  55691      M   55         160          65       85.0             0.9   \n","\n","       eyesight(right)  hearing(left)  hearing(right)  ...  hemoglobin  \\\n","0                  1.0            1.0             1.0  ...        12.9   \n","1                  0.6            1.0             1.0  ...        12.7   \n","2                  0.8            1.0             1.0  ...        15.8   \n","3                  1.5            1.0             1.0  ...        14.7   \n","4                  1.0            1.0             1.0  ...        12.5   \n","...                ...            ...             ...  ...         ...   \n","55687              0.9            1.0             1.0  ...        12.3   \n","55688              1.2            1.0             1.0  ...        14.0   \n","55689              1.2            1.0             1.0  ...        12.4   \n","55690              1.0            1.0             1.0  ...        14.4   \n","55691              0.7            1.0             1.0  ...        15.0   \n","\n","       Urine protein  serum creatinine   AST   ALT   Gtp  oral  dental caries  \\\n","0                1.0               0.7  18.0  19.0  27.0     Y              0   \n","1                1.0               0.6  22.0  19.0  18.0     Y              0   \n","2                1.0               1.0  21.0  16.0  22.0     Y              0   \n","3                1.0               1.0  19.0  26.0  18.0     Y              0   \n","4                1.0               0.6  16.0  14.0  22.0     Y              0   \n","...              ...               ...   ...   ...   ...   ...            ...   \n","55687            1.0               0.6  14.0   7.0  10.0     Y              1   \n","55688            1.0               0.9  20.0  12.0  14.0     Y              0   \n","55689            1.0               0.5  17.0  11.0  12.0     Y              0   \n","55690            1.0               0.7  20.0  19.0  18.0     Y              0   \n","55691            1.0               0.8  26.0  29.0  41.0     Y              0   \n","\n","       tartar  smoking  \n","0           Y        0  \n","1           Y        0  \n","2           N        1  \n","3           Y        0  \n","4           N        0  \n","...       ...      ...  \n","55687       Y        0  \n","55688       Y        0  \n","55689       N        0  \n","55690       N        0  \n","55691       Y        1  \n","\n","[55692 rows x 27 columns]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["smoke_data = pd.read_csv(\"/workspaces/CoderPosta/Data/smoking.csv\")\n","smoke_data"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["df = smoke_data.copy()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Limpieza de Datos"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Nulos y Duplicados"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Se verifica que no hay duplicados ni nulos."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ID                     0\n","dental caries          0\n","oral                   0\n","Gtp                    0\n","ALT                    0\n","AST                    0\n","serum creatinine       0\n","Urine protein          0\n","hemoglobin             0\n","LDL                    0\n","HDL                    0\n","triglyceride           0\n","tartar                 0\n","Cholesterol            0\n","relaxation             0\n","systolic               0\n","hearing(right)         0\n","hearing(left)          0\n","eyesight(right)        0\n","eyesight(left)         0\n","waist(cm)              0\n","weight(kg)             0\n","height(cm)             0\n","age                    0\n","gender                 0\n","fasting blood sugar    0\n","smoking                0\n","dtype: int64\n","Cant. de Duplicados  0\n"]}],"source":["print(df.isna().sum().sort_values())\n","print('Cant. de Duplicados ' ,df.duplicated().sum())"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Agrupacion de Datos"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["ID                       int64\n","gender                  object\n","age                      int64\n","height(cm)               int64\n","weight(kg)               int64\n","waist(cm)              float64\n","eyesight(left)         float64\n","eyesight(right)        float64\n","hearing(left)          float64\n","hearing(right)         float64\n","systolic               float64\n","relaxation             float64\n","fasting blood sugar    float64\n","Cholesterol            float64\n","triglyceride           float64\n","HDL                    float64\n","LDL                    float64\n","hemoglobin             float64\n","Urine protein          float64\n","serum creatinine       float64\n","AST                    float64\n","ALT                    float64\n","Gtp                    float64\n","oral                    object\n","dental caries            int64\n","tartar                  object\n","smoking                  int64\n","dtype: object"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df.dtypes"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Datos  Categóricos"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Categoric Col ['gender', 'oral', 'tartar']\n"]}],"source":["categoric_col = []\n","for col in df.columns:\n","\tif (df[col].nunique()<10) and (df[col].dtype==\"object\"):\n","\t\tcategoric_col.append(col) \n","  \n","print(\"Categoric Col {}\".format(categoric_col))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Datos Continuos"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Cont Col ['hemoglobin', 'age', 'fasting blood sugar', 'LDL', 'systolic', 'eyesight(right)', 'ALT', 'dental caries', 'relaxation', 'hearing(right)', 'hearing(left)', 'serum creatinine', 'weight(kg)', 'ID', 'height(cm)', 'triglyceride', 'smoking', 'Urine protein', 'Cholesterol', 'HDL', 'waist(cm)', 'Gtp', 'AST', 'eyesight(left)']\n"]}],"source":["Numeric=list(set(df.columns)- set(categoric_col))\n","print(\"Cont Col {}\".format(Numeric))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Se identifica que la columna \"oral\" posee una sola variable unica."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["oral                       1\n","smoking                    2\n","gender                     2\n","dental caries              2\n","hearing(left)              2\n","hearing(right)             2\n","tartar                     2\n","Urine protein              6\n","height(cm)                13\n","age                       14\n","eyesight(right)           17\n","eyesight(left)            19\n","weight(kg)                22\n","serum creatinine          38\n","relaxation                95\n","HDL                      126\n","systolic                 130\n","hemoglobin               145\n","AST                      219\n","ALT                      245\n","fasting blood sugar      276\n","Cholesterol              286\n","LDL                      289\n","triglyceride             390\n","Gtp                      488\n","waist(cm)                566\n","ID                     55692\n","dtype: int64"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["df.nunique().sort_values()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Se elimina la variable \"oral\" por su irrelevancia para el modelo."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>gender</th>\n","      <th>age</th>\n","      <th>height(cm)</th>\n","      <th>weight(kg)</th>\n","      <th>waist(cm)</th>\n","      <th>eyesight(left)</th>\n","      <th>eyesight(right)</th>\n","      <th>hearing(left)</th>\n","      <th>hearing(right)</th>\n","      <th>...</th>\n","      <th>LDL</th>\n","      <th>hemoglobin</th>\n","      <th>Urine protein</th>\n","      <th>serum creatinine</th>\n","      <th>AST</th>\n","      <th>ALT</th>\n","      <th>Gtp</th>\n","      <th>dental caries</th>\n","      <th>tartar</th>\n","      <th>smoking</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>F</td>\n","      <td>40</td>\n","      <td>155</td>\n","      <td>60</td>\n","      <td>81.3</td>\n","      <td>1.2</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>126.0</td>\n","      <td>12.9</td>\n","      <td>1.0</td>\n","      <td>0.7</td>\n","      <td>18.0</td>\n","      <td>19.0</td>\n","      <td>27.0</td>\n","      <td>0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>F</td>\n","      <td>40</td>\n","      <td>160</td>\n","      <td>60</td>\n","      <td>81.0</td>\n","      <td>0.8</td>\n","      <td>0.6</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>127.0</td>\n","      <td>12.7</td>\n","      <td>1.0</td>\n","      <td>0.6</td>\n","      <td>22.0</td>\n","      <td>19.0</td>\n","      <td>18.0</td>\n","      <td>0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>M</td>\n","      <td>55</td>\n","      <td>170</td>\n","      <td>60</td>\n","      <td>80.0</td>\n","      <td>0.8</td>\n","      <td>0.8</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>151.0</td>\n","      <td>15.8</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>21.0</td>\n","      <td>16.0</td>\n","      <td>22.0</td>\n","      <td>0</td>\n","      <td>N</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>M</td>\n","      <td>40</td>\n","      <td>165</td>\n","      <td>70</td>\n","      <td>88.0</td>\n","      <td>1.5</td>\n","      <td>1.5</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>226.0</td>\n","      <td>14.7</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>19.0</td>\n","      <td>26.0</td>\n","      <td>18.0</td>\n","      <td>0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>F</td>\n","      <td>40</td>\n","      <td>155</td>\n","      <td>60</td>\n","      <td>86.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>107.0</td>\n","      <td>12.5</td>\n","      <td>1.0</td>\n","      <td>0.6</td>\n","      <td>16.0</td>\n","      <td>14.0</td>\n","      <td>22.0</td>\n","      <td>0</td>\n","      <td>N</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 26 columns</p>\n","</div>"],"text/plain":["   ID gender  age  height(cm)  weight(kg)  waist(cm)  eyesight(left)  \\\n","0   0      F   40         155          60       81.3             1.2   \n","1   1      F   40         160          60       81.0             0.8   \n","2   2      M   55         170          60       80.0             0.8   \n","3   3      M   40         165          70       88.0             1.5   \n","4   4      F   40         155          60       86.0             1.0   \n","\n","   eyesight(right)  hearing(left)  hearing(right)  ...    LDL  hemoglobin  \\\n","0              1.0            1.0             1.0  ...  126.0        12.9   \n","1              0.6            1.0             1.0  ...  127.0        12.7   \n","2              0.8            1.0             1.0  ...  151.0        15.8   \n","3              1.5            1.0             1.0  ...  226.0        14.7   \n","4              1.0            1.0             1.0  ...  107.0        12.5   \n","\n","   Urine protein  serum creatinine   AST   ALT   Gtp  dental caries  tartar  \\\n","0            1.0               0.7  18.0  19.0  27.0              0       Y   \n","1            1.0               0.6  22.0  19.0  18.0              0       Y   \n","2            1.0               1.0  21.0  16.0  22.0              0       N   \n","3            1.0               1.0  19.0  26.0  18.0              0       Y   \n","4            1.0               0.6  16.0  14.0  22.0              0       N   \n","\n","   smoking  \n","0        0  \n","1        0  \n","2        1  \n","3        0  \n","4        0  \n","\n","[5 rows x 26 columns]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["df.drop(labels=\"oral\", axis=1, inplace=True)\n","df.head(5)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Por lo que se observa en los resultados las columnas \"urine protein\", \"hearing(right)\", \"hearing(left)\"y \"dental caries\" no producen informacion valiosa para el modelo asi que se eliminan del \"ds\""]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>gender</th>\n","      <th>age</th>\n","      <th>height(cm)</th>\n","      <th>weight(kg)</th>\n","      <th>waist(cm)</th>\n","      <th>eyesight(left)</th>\n","      <th>eyesight(right)</th>\n","      <th>systolic</th>\n","      <th>relaxation</th>\n","      <th>fasting blood sugar</th>\n","      <th>...</th>\n","      <th>triglyceride</th>\n","      <th>HDL</th>\n","      <th>LDL</th>\n","      <th>hemoglobin</th>\n","      <th>serum creatinine</th>\n","      <th>AST</th>\n","      <th>ALT</th>\n","      <th>Gtp</th>\n","      <th>tartar</th>\n","      <th>smoking</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>F</td>\n","      <td>40</td>\n","      <td>155</td>\n","      <td>60</td>\n","      <td>81.3</td>\n","      <td>1.2</td>\n","      <td>1.0</td>\n","      <td>114.0</td>\n","      <td>73.0</td>\n","      <td>94.0</td>\n","      <td>...</td>\n","      <td>82.0</td>\n","      <td>73.0</td>\n","      <td>126.0</td>\n","      <td>12.9</td>\n","      <td>0.7</td>\n","      <td>18.0</td>\n","      <td>19.0</td>\n","      <td>27.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>F</td>\n","      <td>40</td>\n","      <td>160</td>\n","      <td>60</td>\n","      <td>81.0</td>\n","      <td>0.8</td>\n","      <td>0.6</td>\n","      <td>119.0</td>\n","      <td>70.0</td>\n","      <td>130.0</td>\n","      <td>...</td>\n","      <td>115.0</td>\n","      <td>42.0</td>\n","      <td>127.0</td>\n","      <td>12.7</td>\n","      <td>0.6</td>\n","      <td>22.0</td>\n","      <td>19.0</td>\n","      <td>18.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>M</td>\n","      <td>55</td>\n","      <td>170</td>\n","      <td>60</td>\n","      <td>80.0</td>\n","      <td>0.8</td>\n","      <td>0.8</td>\n","      <td>138.0</td>\n","      <td>86.0</td>\n","      <td>89.0</td>\n","      <td>...</td>\n","      <td>182.0</td>\n","      <td>55.0</td>\n","      <td>151.0</td>\n","      <td>15.8</td>\n","      <td>1.0</td>\n","      <td>21.0</td>\n","      <td>16.0</td>\n","      <td>22.0</td>\n","      <td>N</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>M</td>\n","      <td>40</td>\n","      <td>165</td>\n","      <td>70</td>\n","      <td>88.0</td>\n","      <td>1.5</td>\n","      <td>1.5</td>\n","      <td>100.0</td>\n","      <td>60.0</td>\n","      <td>96.0</td>\n","      <td>...</td>\n","      <td>254.0</td>\n","      <td>45.0</td>\n","      <td>226.0</td>\n","      <td>14.7</td>\n","      <td>1.0</td>\n","      <td>19.0</td>\n","      <td>26.0</td>\n","      <td>18.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>F</td>\n","      <td>40</td>\n","      <td>155</td>\n","      <td>60</td>\n","      <td>86.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>120.0</td>\n","      <td>74.0</td>\n","      <td>80.0</td>\n","      <td>...</td>\n","      <td>74.0</td>\n","      <td>62.0</td>\n","      <td>107.0</td>\n","      <td>12.5</td>\n","      <td>0.6</td>\n","      <td>16.0</td>\n","      <td>14.0</td>\n","      <td>22.0</td>\n","      <td>N</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>55687</th>\n","      <td>F</td>\n","      <td>40</td>\n","      <td>170</td>\n","      <td>65</td>\n","      <td>75.0</td>\n","      <td>0.9</td>\n","      <td>0.9</td>\n","      <td>110.0</td>\n","      <td>68.0</td>\n","      <td>89.0</td>\n","      <td>...</td>\n","      <td>99.0</td>\n","      <td>75.0</td>\n","      <td>118.0</td>\n","      <td>12.3</td>\n","      <td>0.6</td>\n","      <td>14.0</td>\n","      <td>7.0</td>\n","      <td>10.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>55688</th>\n","      <td>F</td>\n","      <td>45</td>\n","      <td>160</td>\n","      <td>50</td>\n","      <td>70.0</td>\n","      <td>1.2</td>\n","      <td>1.2</td>\n","      <td>101.0</td>\n","      <td>62.0</td>\n","      <td>89.0</td>\n","      <td>...</td>\n","      <td>69.0</td>\n","      <td>73.0</td>\n","      <td>79.0</td>\n","      <td>14.0</td>\n","      <td>0.9</td>\n","      <td>20.0</td>\n","      <td>12.0</td>\n","      <td>14.0</td>\n","      <td>Y</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>55689</th>\n","      <td>F</td>\n","      <td>55</td>\n","      <td>160</td>\n","      <td>50</td>\n","      <td>68.5</td>\n","      <td>1.0</td>\n","      <td>1.2</td>\n","      <td>117.0</td>\n","      <td>72.0</td>\n","      <td>88.0</td>\n","      <td>...</td>\n","      <td>77.0</td>\n","      <td>79.0</td>\n","      <td>63.0</td>\n","      <td>12.4</td>\n","      <td>0.5</td>\n","      <td>17.0</td>\n","      <td>11.0</td>\n","      <td>12.0</td>\n","      <td>N</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>55690</th>\n","      <td>M</td>\n","      <td>60</td>\n","      <td>165</td>\n","      <td>60</td>\n","      <td>78.0</td>\n","      <td>0.8</td>\n","      <td>1.0</td>\n","      <td>133.0</td>\n","      <td>76.0</td>\n","      <td>107.0</td>\n","      <td>...</td>\n","      <td>79.0</td>\n","      <td>48.0</td>\n","      <td>146.0</td>\n","      <td>14.4</td>\n","      <td>0.7</td>\n","      <td>20.0</td>\n","      <td>19.0</td>\n","      <td>18.0</td>\n","      <td>N</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>55691</th>\n","      <td>M</td>\n","      <td>55</td>\n","      <td>160</td>\n","      <td>65</td>\n","      <td>85.0</td>\n","      <td>0.9</td>\n","      <td>0.7</td>\n","      <td>124.0</td>\n","      <td>75.0</td>\n","      <td>82.0</td>\n","      <td>...</td>\n","      <td>142.0</td>\n","      <td>34.0</td>\n","      <td>150.0</td>\n","      <td>15.0</td>\n","      <td>0.8</td>\n","      <td>26.0</td>\n","      <td>29.0</td>\n","      <td>41.0</td>\n","      <td>Y</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>55692 rows × 21 columns</p>\n","</div>"],"text/plain":["      gender  age  height(cm)  weight(kg)  waist(cm)  eyesight(left)  \\\n","0          F   40         155          60       81.3             1.2   \n","1          F   40         160          60       81.0             0.8   \n","2          M   55         170          60       80.0             0.8   \n","3          M   40         165          70       88.0             1.5   \n","4          F   40         155          60       86.0             1.0   \n","...      ...  ...         ...         ...        ...             ...   \n","55687      F   40         170          65       75.0             0.9   \n","55688      F   45         160          50       70.0             1.2   \n","55689      F   55         160          50       68.5             1.0   \n","55690      M   60         165          60       78.0             0.8   \n","55691      M   55         160          65       85.0             0.9   \n","\n","       eyesight(right)  systolic  relaxation  fasting blood sugar  ...  \\\n","0                  1.0     114.0        73.0                 94.0  ...   \n","1                  0.6     119.0        70.0                130.0  ...   \n","2                  0.8     138.0        86.0                 89.0  ...   \n","3                  1.5     100.0        60.0                 96.0  ...   \n","4                  1.0     120.0        74.0                 80.0  ...   \n","...                ...       ...         ...                  ...  ...   \n","55687              0.9     110.0        68.0                 89.0  ...   \n","55688              1.2     101.0        62.0                 89.0  ...   \n","55689              1.2     117.0        72.0                 88.0  ...   \n","55690              1.0     133.0        76.0                107.0  ...   \n","55691              0.7     124.0        75.0                 82.0  ...   \n","\n","       triglyceride   HDL    LDL  hemoglobin  serum creatinine   AST   ALT  \\\n","0              82.0  73.0  126.0        12.9               0.7  18.0  19.0   \n","1             115.0  42.0  127.0        12.7               0.6  22.0  19.0   \n","2             182.0  55.0  151.0        15.8               1.0  21.0  16.0   \n","3             254.0  45.0  226.0        14.7               1.0  19.0  26.0   \n","4              74.0  62.0  107.0        12.5               0.6  16.0  14.0   \n","...             ...   ...    ...         ...               ...   ...   ...   \n","55687          99.0  75.0  118.0        12.3               0.6  14.0   7.0   \n","55688          69.0  73.0   79.0        14.0               0.9  20.0  12.0   \n","55689          77.0  79.0   63.0        12.4               0.5  17.0  11.0   \n","55690          79.0  48.0  146.0        14.4               0.7  20.0  19.0   \n","55691         142.0  34.0  150.0        15.0               0.8  26.0  29.0   \n","\n","        Gtp  tartar smoking  \n","0      27.0       Y       0  \n","1      18.0       Y       0  \n","2      22.0       N       1  \n","3      18.0       Y       0  \n","4      22.0       N       0  \n","...     ...     ...     ...  \n","55687  10.0       Y       0  \n","55688  14.0       Y       0  \n","55689  12.0       N       0  \n","55690  18.0       N       0  \n","55691  41.0       Y       1  \n","\n","[55692 rows x 21 columns]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["df.drop(labels=[\"Urine protein\", \"hearing(right)\", \"hearing(left)\", \"dental caries\", \"ID\"], axis=1, inplace=True)\n","df"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Feature Creation"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Se crea la columna Indice de Masa Corporal (IMC) y se decide probar el modelo eliminando sus columnas correlacionadas"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["df[\"height(M)\"] = df[\"height(cm)\"] / 100\n","df[\"IMC\"] = df[\"weight(kg)\"] / (df[\"height(M)\"] ** 2)\n","df = df.drop([\"height(M)\",\"weight(kg)\",\"waist(cm)\"], axis = 1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Se crea la columna Tension Arterial Media (TAM) y se aplica lo mismo que en IMC"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["df['TAM'] = (df['systolic'] + 0.5 * df['relaxation']) / 2\n","df = df.drop([\"systolic\",\"relaxation\"], axis = 1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Se realiza la codificacion de los datos categoricos"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","for col in df.columns:\n","    if df[col].dtype == 'object':\n","        df[col] = le.fit_transform(df[col])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Feature Scaling"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import RobustScaler"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["df_scaled = df.drop([\"gender\",\"tartar\",\"smoking\"], axis=1)\n","scaling = RobustScaler().fit_transform(df_scaled)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["df_scaled = pd.DataFrame(scaling, columns=df_scaled.columns)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["df = df_scaled.join(df[['gender', 'tartar', \"smoking\"]])"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install -q lightgbm"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Feature Selection"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# numpy and pandas para data manipulation\n","import pandas as pd\n","import numpy as np\n","# model usado para feature importances\n","import lightgbm as lgb\n","# Utilidad para hacer separacion en train y test\n","from sklearn.model_selection import train_test_split\n","# visualizaciones\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","# manejo de memoria\n","import gc\n","# utilidades\n","from itertools import chain\n","class FeatureSelector():\n","    \"\"\"\n","    Class para desarrollar  feature selection para algoritmos de machine learning o data preprocessing. \n","    Implementa 5 distintos metodos para identificar features a remover\n","        1. Encuentra las columnas con cierto porcentaje de nulos mayor a un threshold\n","        2. Encuentra las columnas con solo un unico valor\n","        3. Encuentra variables con colinealidad con una correlacion mayor a un valor especificado\n","        4. Encuentra los features con  0.0 en feature importance usando una gradient boosting machine (gbm)\n","        5. Encuentra los valores bajos de feature importance que no ocntribuyen a la cumulative feature importance de la gbm\n","    Parametros\n","    --------\n","        data : dataframe\n","            Un dataset con observaciones y columnas ne las filas\n","        labels : array or series, default = None\n","            Arreglo de labels para entrenar el algoritmo de machine learning model para encontrar las feature importances. \n","            Pueden ser labels binarios (si se trata de 'classification') o valores continuos (si la tarea es 'regression').\n","            Si no se proveen labels el feature importance no estara disponible.\n","    Attributos\n","    --------\n","    ops : dict\n","        Dictionary con las operaciones a correr y los features identificados para remocion\n","    missing_stats : dataframe\n","        La fraccion de missing values de todos los features \n","    record_missing : dataframe\n","        La fracción de valores perdidos para entidades con fracción faltante por encima del umbral\n","    unique_stats : dataframe\n","        Numero de valores unicos para todos los features\n","    record_single_unique : dataframe\n","        Records de los features que solo tienen un valor unico\n","    corr_matrix : dataframe\n","        Todas las correlaciones entre todos los features de la data\n","    record_collinear : dataframe\n","        Registra los pares de variables colineales con un coeficiente de correlación por encima del umbral\n","    feature_importances : dataframe\n","        Todas cuentan los features importantes de la gradient boosting machine\n","    record_zero_importance : dataframe\n","        Registra las características de importancia cero en los datos según el gbm\n","    record_low_importance : dataframe\n","        Registra las características de menor importancia que no son necesarias para alcanzar el umbral de importancia acumulativa según el GBM\n","\n","    Notas\n","    --------\n","    \n","        - Todas las 5 operaciones se pueden correr con el metodo `identify_all`.\n","        - Si usas feature importances, one-hot encoding se usa para las variables categoricas creando nuevas columnas\n","    \"\"\"\n","    \n","    def __init__(self, data, labels=None):    \n","        # Dataset y opcionalmente los training labels\n","        self.data = data\n","        self.labels = labels\n","        if labels is None:\n","            print('No labels provistos. Metodos de Feature importance no estaran disponibles.')\n","        self.base_features = list(data.columns)\n","        self.one_hot_features = None\n","        # Dataframes que guardan la informacion de los features a remover\n","        self.record_missing = None\n","        self.record_single_unique = None\n","        self.record_collinear = None\n","        self.record_zero_importance = None\n","        self.record_low_importance = None   \n","        self.missing_stats = None\n","        self.unique_stats = None\n","        self.corr_matrix = None\n","        self.feature_importances = None\n","        # Dictionary para realizar operaciones de remoción\n","        self.ops = {}\n","        self.one_hot_correlated = False\n","        \n","    def identify_missing(self, missing_threshold):\n","        \"\"\"Encontrar los features con una fraccion de missing values por encima del `missing_threshold`\"\"\"\n","        self.missing_threshold = missing_threshold\n","        # Calcular la fraccion de missing values en cada columna\n","        missing_series = self.data.isnull().sum() / self.data.shape[0]\n","        self.missing_stats = pd.DataFrame(missing_series).rename(columns = {'index': 'feature', 0: 'missing_fraction'})\n","        # Ordenar por el valor mas alto de valores missing en el top\n","        self.missing_stats = self.missing_stats.sort_values('missing_fraction', ascending = False)\n","        # Encontrar las columnas con porcentaje de missing arriba de cierto threshold\n","        record_missing = pd.DataFrame(missing_series[missing_series > missing_threshold]).reset_index().rename(columns = \n","                                                                                                               {'index': 'feature', \n","                                                                                                                0: 'missing_fraction'})\n","        to_drop = list(record_missing['feature'])\n","        self.record_missing = record_missing\n","        self.ops['missing'] = to_drop\n","        print('%d features con cantidad mayor a %0.2f en missing values.\\n' % (len(self.ops['missing']), self.missing_threshold))\n","    def identify_single_unique(self):\n","        \"\"\"Encontrar los features con solo un unico valor. NaNs no cuentan como valor unico. \"\"\"\n","        # Calcular los valores unicos en cada columna\n","        unique_counts = self.data.nunique()\n","        self.unique_stats = pd.DataFrame(unique_counts).rename(columns = {'index': 'feature', 0: 'nunique'})\n","        self.unique_stats = self.unique_stats.sort_values('nunique', ascending = True)        \n","        # Encontrar las columnas con solo un unico valor \n","        record_single_unique = pd.DataFrame(unique_counts[unique_counts == 1]).reset_index().rename(columns = {'index': 'feature', \n","                                                                                                                0: 'nunique'})\n","        to_drop = list(record_single_unique['feature'])\n","        self.record_single_unique = record_single_unique\n","        self.ops['single_unique'] = to_drop\n","        \n","        print('%d features con un valor unico .\\n' % len(self.ops['single_unique']))\n","    def identify_collinear(self, correlation_threshold, one_hot=False):\n","        \"\"\"\n","        Encuentra características colineales según el coeficiente de correlación entre características.\n","         Para cada par de características con un coeficiente de correlación mayor que \"correlation_threshold\",\n","         sólo uno de la pareja se identifica para eliminacion        \n","        Este codigo esta adaptado de: https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/\n","        Parametros\n","        --------\n","        correlation_threshold : float between 0 and 1\n","            Valor de la correlacion de  Pearson para identificat relaciones\n","        one_hot : boolean, default = False\n","            Cuando desee usar one-hot encode para los features antes de calcular los coef. de correlacion\n","        \"\"\"\n","        self.correlation_threshold = correlation_threshold\n","        self.one_hot_correlated = one_hot\n","         # Calcular la correlaciones entre cada columna\n","        if one_hot:\n","            # One hot encoding\n","            features = pd.get_dummies(self.data)\n","            self.one_hot_features = [column for column in features.columns if column not in self.base_features]\n","            # Agregar one hot encoded data a la data original\n","            self.data_all = pd.concat([features[self.one_hot_features], self.data], axis = 1)\n","            corr_matrix = pd.get_dummies(features).corr()\n","        else:\n","            corr_matrix = self.data.corr()\n","        self.corr_matrix = corr_matrix\n","    \n","        # Extraer el triangulo superior derecho de la matriz de correlacion\n","        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n","        # Seleccionar los features con correlacion por encima de un threshold\n","        # Se necesita usar valor absoluto \n","        to_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\n","        # Dataframe para almacenar las parejas correlacionadas\n","        record_collinear = pd.DataFrame(columns = ['drop_feature', 'corr_feature', 'corr_value'])\n","        # Iterar sobre las columnas para eliminar los records de parejas con features correlacionados\n","        for column in to_drop:\n","            #Encontrar los features correlacionados\n","            corr_features = list(upper.index[upper[column].abs() > correlation_threshold])\n","            #Encontrar los valores correlacionados\n","            corr_values = list(upper[column][upper[column].abs() > correlation_threshold])\n","            drop_features = [column for _ in range(len(corr_features))]   \n","            # guardar la info (usamos un temp df por ahora)\n","            temp_df = pd.DataFrame.from_dict({'drop_feature': drop_features,\n","                                             'corr_feature': corr_features,\n","                                             'corr_value': corr_values})\n","            # Agregar a un data frame\n","            record_collinear = record_collinear.append(temp_df, ignore_index = True)\n","        self.record_collinear = record_collinear\n","        self.ops['collinear'] = to_drop\n","        \n","        print('%d features con una mangitud de correlacion mayor que %0.2f.\\n' % (len(self.ops['collinear']), self.correlation_threshold))\n","\n","    def identify_zero_importance(self, task, eval_metric=None, \n","                                 n_iterations=10, early_stopping = True):\n","        \"\"\"\n","        \n","        Identificar los features con 0 importancia acorde con la gradient boosting machine.\n","        El gbm puede ser entrenado con un parametro llamado early stopping usando un test de validacion para prevenir el overfitting. \n","        Las feature importances se promedian sobre todas las `n_iterations` para reducir la varianza. \n","        Uso de la implementacion LightGBM, pueden leer: (http://lightgbm.readthedocs.io/en/latest/index.html)\n","        Parametros\n","        -------\n","        eval_metric : string\n","            Metrica de evaluacion usada para el gradient boosting con early stopping. Debe proveerse `early_stopping` is True\n","        task : string\n","            La machine learning task, puede ser 'classification' o 'regression'\n","        n_iterations : int, default = 10\n","            Numero de iteraciones para la gradient boosting machine\n","        early_stopping : boolean, default = True\n","            Cuando o no usar early stopping con validation set cuendo se entrene\n","        Notes\n","        --------\n","        - Features se les aplica one-hot encoded para manipular variables categoricas antes de training.\n","        - El gbm no es optimizado para una tarea particular y puede ser necesario algun hyperparameter tuning\n","        - Las feature importances, incluyendo los importance features 0 ,pueden cambiar en diferentes corridas pero no mucho\n","        \"\"\"\n","        if early_stopping and eval_metric is None:\n","            raise ValueError(\"\"\"eval metric debe proveerse con early stopping. Algunos ejemplos \"auc\" para classification o\n","                             \"l2\"para regression.\"\"\")\n","            \n","        if self.labels is None:\n","            raise ValueError(\"No training labels se proveen.\")\n","        \n","        # One hot encoding\n","        features = pd.get_dummies(self.data)\n","        self.one_hot_features = [column for column in features.columns if column not in self.base_features]\n","        # Agregar la data one hot encoded a la data original\n","        self.data_all = pd.concat([features[self.one_hot_features], self.data], axis = 1)\n","        # Extraer los feature names\n","        feature_names = list(features.columns)\n","        # Convertir a np array\n","        features = np.array(features)\n","        labels = np.array(self.labels).reshape((-1, ))\n","        # Empty array para feature importances\n","        feature_importance_values = np.zeros(len(feature_names))\n","        print('Entrenar el Gradient Boosting Model\\n')\n","        # Iterate sobre cada fold\n","        for _ in range(n_iterations):\n","            if task == 'classification':\n","                model = lgb.LGBMClassifier(n_estimators=1000, learning_rate = 0.05, verbose = -1)\n","            elif task == 'regression':\n","                model = lgb.LGBMRegressor(n_estimators=1000, learning_rate = 0.05, verbose = -1)\n","            else:\n","                raise ValueError('La tarea deber ser \"classification\" o \"regression\"')\n","            # Si en entrenamiento se usa early stopping se necesita validation set\n","            if early_stopping:\n","                train_features, valid_features, train_labels, valid_labels = train_test_split(features, labels, test_size = 0.15, stratify=labels)\n","                # Entrenar el modelo con early stopping\n","                model.fit(train_features, train_labels, eval_metric = eval_metric,\n","                          eval_set = [(valid_features, valid_labels)],\n","                          early_stopping_rounds = 100, verbose = -1)\n","                # Limpiar memory\n","                gc.enable()\n","                del train_features, train_labels, valid_features, valid_labels\n","                gc.collect()\n","            else:\n","                model.fit(features, labels)\n","            # Guardar los feature importances\n","            feature_importance_values += model.feature_importances_ / n_iterations\n","        feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n","        # Ordenar los features de acuerdo a su importancia\n","        feature_importances = feature_importances.sort_values('importance', ascending = False).reset_index(drop = True)\n","        # Normalizar las feature importances para que sumados den 1\n","        feature_importances['normalized_importance'] = feature_importances['importance'] / feature_importances['importance'].sum()\n","        feature_importances['cumulative_importance'] = np.cumsum(feature_importances['normalized_importance'])\n","        # Extraer los features con 0 importancia\n","        record_zero_importance = feature_importances[feature_importances['importance'] == 0.0]\n","        to_drop = list(record_zero_importance['feature'])\n","        self.feature_importances = feature_importances\n","        self.record_zero_importance = record_zero_importance\n","        self.ops['zero_importance'] = to_drop\n","        print('\\n%d features con cero importancia despues de one-hot encoding.\\n' % len(self.ops['zero_importance']))\n","    \n","    def identify_low_importance(self, cumulative_importance):\n","        \"\"\"\n","        Encontrar los fetures con mas bajo importance usando `cumulative_importance` \n","        del total de importancia obtenido del gradient boosting machine. Por ejemplo si la importancia acumulada\n","        es 0.95, esto retendra solo los features mas importantes necesarios para alcanzar\n","        95% del total de feature importance. \n","        Parametros\n","        --------\n","        cumulative_importance : float between 0 and 1\n","            Fraccion de la importancia acumulada a tener en cuenta\n","        \"\"\"\n","        self.cumulative_importance = cumulative_importance\n","        # Las feature importances se deben calcular primero antes de correr \n","        if self.feature_importances is None:\n","            raise NotImplementedError(\"\"\"Feature importances aun no se han determinado. \n","                                         Llamar al metodo `identify_zero_importance` primero.\"\"\")\n","        #Asegurarse que los features mas importantes estan en el top\n","        self.feature_importances = self.feature_importances.sort_values('cumulative_importance')\n","        # Identificar los features que no son necesarios para alcanzar la cumulative_importance\n","        record_low_importance = self.feature_importances[self.feature_importances['cumulative_importance'] > cumulative_importance]\n","        to_drop = list(record_low_importance['feature'])\n","        self.record_low_importance = record_low_importance\n","        self.ops['low_importance'] = to_drop\n","        print('%d features requeridos para el cumulative importance de %0.2f luego one hot encoding.' % (len(self.feature_importances) -\n","                                                                            len(self.record_low_importance), self.cumulative_importance))\n","        print('%d features que no contribuyen a la cumulative importance de %0.2f.\\n' % (len(self.ops['low_importance']),\n","                                                                                               self.cumulative_importance))\n","    def identify_all(self, selection_params):\n","        \"\"\"\n","        Usar los 5 metodos para identificar los nombres de features a remover\n","        Parametros\n","        --------\n","        selection_params : dict\n","           Parametros a usar en los 5 feature selection methhods.\n","           Params puede contener los keys ['missing_threshold', 'correlation_threshold', 'eval_metric', 'task', 'cumulative_importance']\n","        \n","        \"\"\"\n","        # Check de los parametros requeridos\n","        for param in ['missing_threshold', 'correlation_threshold', 'eval_metric', 'task', 'cumulative_importance']:\n","            if param not in selection_params.keys():\n","                raise ValueError('%s se requiere un parametro para este metodo.' % param)\n","        # Implementar cada uno de los 5 metodos\n","        self.identify_missing(selection_params['missing_threshold'])\n","        self.identify_single_unique()\n","        self.identify_collinear(selection_params['correlation_threshold'])\n","        self.identify_zero_importance(task = selection_params['task'], eval_metric = selection_params['eval_metric'])\n","        self.identify_low_importance(selection_params['cumulative_importance'])\n","        # Encontrar el numero de features a eliminar \n","        self.all_identified = set(list(chain(*list(self.ops.values()))))\n","        self.n_identified = len(self.all_identified)  \n","        print('%d total de features en %d identificados para remocion despues de one-hot encoding.\\n' % (self.n_identified, \n","                                                                                                  self.data_all.shape[1]))\n","    def check_removal(self, keep_one_hot=True):\n","        \"\"\"Check los features identificados antes de remover. Devuelve una lista de features unicos identificados.\"\"\"\n","        self.all_identified = set(list(chain(*list(self.ops.values()))))\n","        print('Total de %d features identificados para remocion' % len(self.all_identified))\n","        if not keep_one_hot:\n","            if self.one_hot_features is None:\n","                print('Data no se le ha aplicado one-hot encoded')\n","            else:\n","                one_hot_to_remove = [x for x in self.one_hot_features if x not in self.all_identified]\n","                print('%d one-hot features adicionales que pueden removerse' % len(one_hot_to_remove))\n","        return list(self.all_identified)\n","        \n","    def remove(self, methods, keep_one_hot = True):\n","        \"\"\"\n","        Elimine las características de los datos de acuerdo con los métodos especificados.\n","        \n","         Parámetros\n","        --------\n","            methods : 'all' or list of methods\n","                If methods == 'all', todos los metodos seran usados. En otro caso solo la seleccion que provea.\n","                Opciones: ['missing', 'single_unique', 'collinear', 'zero_importance', 'low_importance']\n","            keep_one_hot : boolean, default = True\n","                Cuando o no mantener los features de one-hot encoded \n","        Return\n","        --------\n","            data : dataframe\n","                Dataframe con los features identificados removidos \n","        Notes \n","        --------\n","            - Si se utilizan características importantes, las columnas codificadas en un solo uso se agregarán a los datos (y luego se pueden eliminar)\n","             - ¡Compruebe las funciones que eliminarán antes de transformar los datos!\n","        \"\"\"\n","        features_to_drop = []\n","        if methods == 'all':\n","            # Necesita el uso de  one-hot encoded data \n","            data = self.data_all\n","            print('{} metodos que se han utilizado\\n'.format(list(self.ops.keys())))\n","            # Encontrar los features unicos a remover\n","            features_to_drop = set(list(chain(*list(self.ops.values()))))\n","        else:\n","            # Necesita el uso de  one-hot encoded \n","            if 'zero_importance' in methods or 'low_importance' in methods or self.one_hot_correlated:\n","                data = self.data_all\n","            else:\n","                data = self.data\n","            # Iterar sobre los metodos especificos \n","            for method in methods:\n","                # Check del metodo que se ha usad\n","                if method not in self.ops.keys():\n","                    raise NotImplementedError('%s method no ha sido implementado ' % method)\n","                # Agregar a la lista los features identificados a remover\n","                else:\n","                    features_to_drop.append(self.ops[method])\n","            # Encontrar los features unicos a remover \n","            features_to_drop = set(list(chain(*features_to_drop)))\n","        features_to_drop = list(features_to_drop)\n","        if not keep_one_hot:\n","            if self.one_hot_features is None:\n","                print('Data no ha sido one-hot encoded')\n","            else: \n","                features_to_drop = list(set(features_to_drop) | set(self.one_hot_features))\n","        # Remover los features y devolver la data\n","        data = data.drop(columns = features_to_drop)\n","        self.removed_features = features_to_drop\n","        if not keep_one_hot:\n","        \tprint('Removidos %d features incluyendo one-hot features.' % len(features_to_drop))\n","        else:\n","        \tprint('Removidos %d features.' % len(features_to_drop))\n","        return data\n","    \n","    def plot_missing(self):\n","        \"\"\"Histograma de la fraccion de missing en cada features\"\"\"\n","        if self.record_missing is None:\n","            raise NotImplementedError(\"Missing values no han sido calculados aun. Run `identify_missing`\")\n","        self.reset_plot()\n","        # Histograma de missing values\n","        plt.style.use('seaborn-white')\n","        plt.figure(figsize = (7, 5))\n","        plt.hist(self.missing_stats['missing_fraction'], bins = np.linspace(0, 1, 11), edgecolor = 'k', color = 'red', linewidth = 1.5)\n","        plt.xticks(np.linspace(0, 1, 11));\n","        plt.xlabel('Missing Fraction', size = 14); plt.ylabel('Count of Features', size = 14); \n","        plt.title(\"Fraction of Missing Values Histogram\", size = 16);\n","    def plot_unique(self):\n","        \"\"\"Histograma de valores unicos para cada feature\"\"\"\n","        if self.record_single_unique is None:\n","            raise NotImplementedError('Unique values no han sido calculados. Run `identify_single_unique`')\n","        self.reset_plot()\n","        # Histograma de valores unicos\n","        self.unique_stats.plot.hist(edgecolor = 'k', figsize = (7, 5))\n","        plt.ylabel('Frequency', size = 14); plt.xlabel('Unique Values', size = 14); \n","        plt.title('Number of Unique Values Histogram', size = 16);\n","    def plot_collinear(self, plot_all = False):\n","        \"\"\"\n","       Mapa de calor de los valores de correlación. Si plot_all = True traza todas las correlaciones de lo contrario\n","         traza solo aquellas características que tienen una correlación por encima del umbral \n","        Notas\n","        --------\n","            - No todas las correlaciones trazadas están por encima del umbral porque en esta grafica\n","             todas las variables que han sido identificadas por tener incluso una correlación por encima del umbral\n","             - Las características del eje x son las que se eliminarán. Las características en el eje y\n","             son las características correlacionadas con las del eje x\n","        Codigo adaptado de https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n","        \"\"\"\n","        if self.record_collinear is None:\n","            raise NotImplementedError('Collinear features have no se han identificado. Run `identify_collinear`.')\n","        if plot_all:\n","        \tcorr_matrix_plot = self.corr_matrix\n","        \ttitle = 'All Correlations'\n","        else:\n","\t        # Identificar correlaciones por encima de un threshold\n","\t        # columnas (x-axis) son los features a eliminar y rows (y_axis) son las parejas correlacionadas\n","\t        corr_matrix_plot = self.corr_matrix.loc[list(set(self.record_collinear['corr_feature'])), \n","\t                                                list(set(self.record_collinear['drop_feature']))]\n","\t        title = \"Correlations Above Threshold\"  \n","        f, ax = plt.subplots(figsize=(10, 8))\n","        # Diverging colormap\n","        cmap = sns.diverging_palette(220, 10, as_cmap=True)\n","        # Hacer el heatmap con el color bar\n","        sns.heatmap(corr_matrix_plot, cmap=cmap, center=0,\n","                    linewidths=.25, cbar_kws={\"shrink\": 0.6})\n","        # Ajustar ylabels \n","        ax.set_yticks([x + 0.5 for x in list(range(corr_matrix_plot.shape[0]))])\n","        ax.set_yticklabels(list(corr_matrix_plot.index), size = int(160 / corr_matrix_plot.shape[0]));\n","        # Ajustar xlabels \n","        ax.set_xticks([x + 0.5 for x in list(range(corr_matrix_plot.shape[1]))])\n","        ax.set_xticklabels(list(corr_matrix_plot.columns), size = int(160 / corr_matrix_plot.shape[1]));\n","        plt.title(title, size = 14)\n","    def plot_feature_importances(self, plot_n = 15, threshold = None):\n","        \"\"\"\n","        Plots `plot_n` los features mas importantes teniendo enc uenta la cumulative importance.\n","        Si `threshold` se provee, imprime el numero de features necesarios para alcanzar el `threshold` de cumulative importance.\n","        Parametros\n","        --------\n","        plot_n : int, default = 15\n","            Número de características más importantes para graficar. El valor predeterminado es 15 o el número máximo de funciones, lo que sea menor.\n","        threshold : float, between 0 and 1 default = None\n","            Umbral para imprimir información sobre importancias acumulativas\n","        \"\"\"\n","        if self.record_zero_importance is None:\n","            raise NotImplementedError('Feature importances no han sido determinadas. Run `idenfity_zero_importance`')\n","        # Necesita ajustar el número de características si es mayor que las características en los datos\n","        if plot_n > self.feature_importances.shape[0]:\n","            plot_n = self.feature_importances.shape[0] - 1\n","        self.reset_plot()\n","        # Hacer un gráfico de barras horizontales de la importancia de las características\n","        plt.figure(figsize = (10, 6))\n","        ax = plt.subplot()\n","        # Es necesario invertir el índice para trazar los más importantes en la parte superior.\n","         # Podría haber un método más eficiente para lograr esto\n","        ax.barh(list(reversed(list(self.feature_importances.index[:plot_n]))), \n","                self.feature_importances['normalized_importance'][:plot_n], \n","                align = 'center', edgecolor = 'k')\n","        # Set los yticks t labels\n","        ax.set_yticks(list(reversed(list(self.feature_importances.index[:plot_n]))))\n","        ax.set_yticklabels(self.feature_importances['feature'][:plot_n], size = 12)\n","        # Plot\n","        plt.xlabel('Normalized Importance', size = 16); plt.title('Feature Importances', size = 18)\n","        plt.show()\n","        # Cumulative importance plot\n","        plt.figure(figsize = (6, 4))\n","        plt.plot(list(range(1, len(self.feature_importances) + 1)), self.feature_importances['cumulative_importance'], 'r-')\n","        plt.xlabel('Number of Features', size = 14); plt.ylabel('Cumulative Importance', size = 14); \n","        plt.title('Cumulative Feature Importance', size = 16);\n","        if threshold:\n","            # Índice de la cantidad mínima de características necesarias para el umbral de importancia acumulativa\n","             # np.where devuelve el índice, por lo que es necesario agregar 1 para tener el número correcto\n","            importance_index = np.min(np.where(self.feature_importances['cumulative_importance'] > threshold))\n","            plt.vlines(x = importance_index + 1, ymin = 0, ymax = 1, linestyles='--', colors = 'blue')\n","            plt.show();\n","            print('%d features requidos para un %0.2f de cumulative importance' % (importance_index + 1, threshold))\n","    def reset_plot(self):\n","        plt.rcParams = plt.rcParamsDefault"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["se realiza la subdivision de x e y en archivo con limpieza y sin limpieza"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["x = df.drop(\"smoking\", axis=1)      \n","y = df['smoking']"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["from sklearn.ensemble import IsolationForest\n","\n","# Identificar outliers en el dataset de entrenamiento\n","iso = IsolationForest(contamination=0.2) # contamination = proporción de outliers esperada\n","yhat = iso.fit_predict(x)\n","\n","# Seleccionar todas las filas que no son outliers\n","mask = yhat != -1 # filtro\n","x = x.loc[mask, :]\n","y = y.loc[mask]\n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0 features con cantidad mayor a 0.70 en missing values.\n","\n","0 features con un valor unico .\n","\n","0 features con una mangitud de correlacion mayor que 0.91.\n","\n","Entrenar el Gradient Boosting Model\n","\n"]}],"source":["fs = FeatureSelector(data = x, labels = y)\n","\n","fs.identify_all(selection_params = {'missing_threshold': 0.7, 'correlation_threshold': 0.91, \n","                                    'task': 'classification', 'eval_metric': 'auc', \n","                                     'cumulative_importance': 0.99})"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Se remueven las columnas determinadas por el Feature selection"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['missing', 'single_unique', 'collinear', 'zero_importance', 'low_importance'] metodos que se han utilizado\n","\n","Removidos 2 features.\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>height(cm)</th>\n","      <th>eyesight(left)</th>\n","      <th>eyesight(right)</th>\n","      <th>fasting blood sugar</th>\n","      <th>Cholesterol</th>\n","      <th>triglyceride</th>\n","      <th>HDL</th>\n","      <th>LDL</th>\n","      <th>hemoglobin</th>\n","      <th>serum creatinine</th>\n","      <th>AST</th>\n","      <th>ALT</th>\n","      <th>Gtp</th>\n","      <th>IMC</th>\n","      <th>TAM</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000000</td>\n","      <td>-1.0</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>-0.133333</td>\n","      <td>0.416667</td>\n","      <td>-0.302326</td>\n","      <td>0.947368</td>\n","      <td>0.295455</td>\n","      <td>-0.863636</td>\n","      <td>-1.0</td>\n","      <td>-0.555556</td>\n","      <td>-0.1250</td>\n","      <td>0.076923</td>\n","      <td>0.243247</td>\n","      <td>-0.369565</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000000</td>\n","      <td>-0.5</td>\n","      <td>-0.50</td>\n","      <td>-1.00</td>\n","      <td>2.033333</td>\n","      <td>-0.062500</td>\n","      <td>0.081395</td>\n","      <td>-0.684211</td>\n","      <td>0.318182</td>\n","      <td>-0.954545</td>\n","      <td>-1.5</td>\n","      <td>-0.111111</td>\n","      <td>-0.1250</td>\n","      <td>-0.269231</td>\n","      <td>-0.096871</td>\n","      <td>-0.217391</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.000000</td>\n","      <td>0.5</td>\n","      <td>-0.50</td>\n","      <td>-0.50</td>\n","      <td>-0.466667</td>\n","      <td>0.979167</td>\n","      <td>0.860465</td>\n","      <td>0.000000</td>\n","      <td>0.863636</td>\n","      <td>0.454545</td>\n","      <td>0.5</td>\n","      <td>-0.222222</td>\n","      <td>-0.3125</td>\n","      <td>-0.115385</td>\n","      <td>-0.689289</td>\n","      <td>0.956522</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>1.25</td>\n","      <td>1.25</td>\n","      <td>0.000000</td>\n","      <td>2.020833</td>\n","      <td>1.697674</td>\n","      <td>-0.526316</td>\n","      <td>2.022727</td>\n","      <td>-0.045455</td>\n","      <td>0.5</td>\n","      <td>-0.444444</td>\n","      <td>0.3125</td>\n","      <td>-0.269231</td>\n","      <td>0.406540</td>\n","      <td>-1.260870</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.000000</td>\n","      <td>-1.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>-1.066667</td>\n","      <td>-0.229167</td>\n","      <td>-0.395349</td>\n","      <td>0.368421</td>\n","      <td>-0.136364</td>\n","      <td>-1.045455</td>\n","      <td>-1.5</td>\n","      <td>-0.777778</td>\n","      <td>-0.4375</td>\n","      <td>-0.115385</td>\n","      <td>0.243247</td>\n","      <td>-0.086957</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>55687</th>\n","      <td>0.000000</td>\n","      <td>0.5</td>\n","      <td>-0.25</td>\n","      <td>-0.25</td>\n","      <td>-0.466667</td>\n","      <td>0.375000</td>\n","      <td>-0.104651</td>\n","      <td>1.052632</td>\n","      <td>0.113636</td>\n","      <td>-1.136364</td>\n","      <td>-1.5</td>\n","      <td>-1.000000</td>\n","      <td>-0.8750</td>\n","      <td>-0.576923</td>\n","      <td>-0.306311</td>\n","      <td>-0.652174</td>\n","    </tr>\n","    <tr>\n","      <th>55688</th>\n","      <td>0.333333</td>\n","      <td>-0.5</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>-0.466667</td>\n","      <td>-0.604167</td>\n","      <td>-0.453488</td>\n","      <td>0.947368</td>\n","      <td>-0.772727</td>\n","      <td>-0.363636</td>\n","      <td>0.0</td>\n","      <td>-0.333333</td>\n","      <td>-0.5625</td>\n","      <td>-0.423077</td>\n","      <td>-0.961562</td>\n","      <td>-1.173913</td>\n","    </tr>\n","    <tr>\n","      <th>55689</th>\n","      <td>1.000000</td>\n","      <td>-0.5</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>-0.533333</td>\n","      <td>-0.770833</td>\n","      <td>-0.360465</td>\n","      <td>1.263158</td>\n","      <td>-1.136364</td>\n","      <td>-1.090909</td>\n","      <td>-2.0</td>\n","      <td>-0.666667</td>\n","      <td>-0.6250</td>\n","      <td>-0.500000</td>\n","      <td>-0.961562</td>\n","      <td>-0.260870</td>\n","    </tr>\n","    <tr>\n","      <th>55690</th>\n","      <td>1.333333</td>\n","      <td>0.0</td>\n","      <td>-0.50</td>\n","      <td>0.00</td>\n","      <td>0.733333</td>\n","      <td>0.312500</td>\n","      <td>-0.337209</td>\n","      <td>-0.368421</td>\n","      <td>0.750000</td>\n","      <td>-0.181818</td>\n","      <td>-1.0</td>\n","      <td>-0.333333</td>\n","      <td>-0.1250</td>\n","      <td>-0.269231</td>\n","      <td>-0.406540</td>\n","      <td>0.521739</td>\n","    </tr>\n","    <tr>\n","      <th>55691</th>\n","      <td>1.000000</td>\n","      <td>-0.5</td>\n","      <td>-0.25</td>\n","      <td>-0.75</td>\n","      <td>-0.933333</td>\n","      <td>0.375000</td>\n","      <td>0.395349</td>\n","      <td>-1.105263</td>\n","      <td>0.840909</td>\n","      <td>0.090909</td>\n","      <td>-0.5</td>\n","      <td>0.333333</td>\n","      <td>0.5000</td>\n","      <td>0.615385</td>\n","      <td>0.335475</td>\n","      <td>0.108696</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>55692 rows × 16 columns</p>\n","</div>"],"text/plain":["            age  height(cm)  eyesight(left)  eyesight(right)  \\\n","0      0.000000        -1.0            0.50             0.00   \n","1      0.000000        -0.5           -0.50            -1.00   \n","2      1.000000         0.5           -0.50            -0.50   \n","3      0.000000         0.0            1.25             1.25   \n","4      0.000000        -1.0            0.00             0.00   \n","...         ...         ...             ...              ...   \n","55687  0.000000         0.5           -0.25            -0.25   \n","55688  0.333333        -0.5            0.50             0.50   \n","55689  1.000000        -0.5            0.00             0.50   \n","55690  1.333333         0.0           -0.50             0.00   \n","55691  1.000000        -0.5           -0.25            -0.75   \n","\n","       fasting blood sugar  Cholesterol  triglyceride       HDL       LDL  \\\n","0                -0.133333     0.416667     -0.302326  0.947368  0.295455   \n","1                 2.033333    -0.062500      0.081395 -0.684211  0.318182   \n","2                -0.466667     0.979167      0.860465  0.000000  0.863636   \n","3                 0.000000     2.020833      1.697674 -0.526316  2.022727   \n","4                -1.066667    -0.229167     -0.395349  0.368421 -0.136364   \n","...                    ...          ...           ...       ...       ...   \n","55687            -0.466667     0.375000     -0.104651  1.052632  0.113636   \n","55688            -0.466667    -0.604167     -0.453488  0.947368 -0.772727   \n","55689            -0.533333    -0.770833     -0.360465  1.263158 -1.136364   \n","55690             0.733333     0.312500     -0.337209 -0.368421  0.750000   \n","55691            -0.933333     0.375000      0.395349 -1.105263  0.840909   \n","\n","       hemoglobin  serum creatinine       AST     ALT       Gtp       IMC  \\\n","0       -0.863636              -1.0 -0.555556 -0.1250  0.076923  0.243247   \n","1       -0.954545              -1.5 -0.111111 -0.1250 -0.269231 -0.096871   \n","2        0.454545               0.5 -0.222222 -0.3125 -0.115385 -0.689289   \n","3       -0.045455               0.5 -0.444444  0.3125 -0.269231  0.406540   \n","4       -1.045455              -1.5 -0.777778 -0.4375 -0.115385  0.243247   \n","...           ...               ...       ...     ...       ...       ...   \n","55687   -1.136364              -1.5 -1.000000 -0.8750 -0.576923 -0.306311   \n","55688   -0.363636               0.0 -0.333333 -0.5625 -0.423077 -0.961562   \n","55689   -1.090909              -2.0 -0.666667 -0.6250 -0.500000 -0.961562   \n","55690   -0.181818              -1.0 -0.333333 -0.1250 -0.269231 -0.406540   \n","55691    0.090909              -0.5  0.333333  0.5000  0.615385  0.335475   \n","\n","            TAM  \n","0     -0.369565  \n","1     -0.217391  \n","2      0.956522  \n","3     -1.260870  \n","4     -0.086957  \n","...         ...  \n","55687 -0.652174  \n","55688 -1.173913  \n","55689 -0.260870  \n","55690  0.521739  \n","55691  0.108696  \n","\n","[55692 rows x 16 columns]"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["x = fs.remove(methods = 'all', keep_one_hot = True)\n","x"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Preparacion de particion de testeo y de entrenamiento para ambos DF"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","x_train , x_test , y_train , y_test = train_test_split(x , y  ,test_size = 0.20 , random_state = 42)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["((44553, 16), (44553,), (11139, 16), (11139,))"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["x_train.shape , y_train.shape , x_test.shape , y_test.shape "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Confeccion de modelo"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["En esta version se esta probando Random Forest Para contrastar con arbol de decicion"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=6, n_estimators=30, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=6, n_estimators=30, random_state=42)</pre></div></div></div></div></div>"],"text/plain":["RandomForestClassifier(max_depth=6, n_estimators=30, random_state=42)"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.ensemble import RandomForestClassifier\n","clf = RandomForestClassifier(max_depth=6, random_state=42, n_estimators=30,criterion='gini')\n","clf.fit(x_train, y_train)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Hypertuning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["random_forest = RandomForestClassifier()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0;31mInit signature:\u001b[0m\n","\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gini'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sqrt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mmax_leaf_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mmin_impurity_decrease\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mbootstrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0moob_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mccp_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mDocstring:\u001b[0m     \n","A random forest classifier.\n","\n","A random forest is a meta estimator that fits a number of decision tree\n","classifiers on various sub-samples of the dataset and uses averaging to\n","improve the predictive accuracy and control over-fitting.\n","The sub-sample size is controlled with the `max_samples` parameter if\n","`bootstrap=True` (default), otherwise the whole dataset is used to build\n","each tree.\n","\n","Read more in the :ref:`User Guide <forest>`.\n","\n","Parameters\n","----------\n","n_estimators : int, default=100\n","    The number of trees in the forest.\n","\n","    .. versionchanged:: 0.22\n","       The default value of ``n_estimators`` changed from 10 to 100\n","       in 0.22.\n","\n","criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n","    The function to measure the quality of a split. Supported criteria are\n","    \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n","    Shannon information gain, see :ref:`tree_mathematical_formulation`.\n","    Note: This parameter is tree-specific.\n","\n","max_depth : int, default=None\n","    The maximum depth of the tree. If None, then nodes are expanded until\n","    all leaves are pure or until all leaves contain less than\n","    min_samples_split samples.\n","\n","min_samples_split : int or float, default=2\n","    The minimum number of samples required to split an internal node:\n","\n","    - If int, then consider `min_samples_split` as the minimum number.\n","    - If float, then `min_samples_split` is a fraction and\n","      `ceil(min_samples_split * n_samples)` are the minimum\n","      number of samples for each split.\n","\n","    .. versionchanged:: 0.18\n","       Added float values for fractions.\n","\n","min_samples_leaf : int or float, default=1\n","    The minimum number of samples required to be at a leaf node.\n","    A split point at any depth will only be considered if it leaves at\n","    least ``min_samples_leaf`` training samples in each of the left and\n","    right branches.  This may have the effect of smoothing the model,\n","    especially in regression.\n","\n","    - If int, then consider `min_samples_leaf` as the minimum number.\n","    - If float, then `min_samples_leaf` is a fraction and\n","      `ceil(min_samples_leaf * n_samples)` are the minimum\n","      number of samples for each node.\n","\n","    .. versionchanged:: 0.18\n","       Added float values for fractions.\n","\n","min_weight_fraction_leaf : float, default=0.0\n","    The minimum weighted fraction of the sum total of weights (of all\n","    the input samples) required to be at a leaf node. Samples have\n","    equal weight when sample_weight is not provided.\n","\n","max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n","    The number of features to consider when looking for the best split:\n","\n","    - If int, then consider `max_features` features at each split.\n","    - If float, then `max_features` is a fraction and\n","      `max(1, int(max_features * n_features_in_))` features are considered at each\n","      split.\n","    - If \"auto\", then `max_features=sqrt(n_features)`.\n","    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n","    - If \"log2\", then `max_features=log2(n_features)`.\n","    - If None, then `max_features=n_features`.\n","\n","    .. versionchanged:: 1.1\n","        The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n","\n","    .. deprecated:: 1.1\n","        The `\"auto\"` option was deprecated in 1.1 and will be removed\n","        in 1.3.\n","\n","    Note: the search for a split does not stop until at least one\n","    valid partition of the node samples is found, even if it requires to\n","    effectively inspect more than ``max_features`` features.\n","\n","max_leaf_nodes : int, default=None\n","    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n","    Best nodes are defined as relative reduction in impurity.\n","    If None then unlimited number of leaf nodes.\n","\n","min_impurity_decrease : float, default=0.0\n","    A node will be split if this split induces a decrease of the impurity\n","    greater than or equal to this value.\n","\n","    The weighted impurity decrease equation is the following::\n","\n","        N_t / N * (impurity - N_t_R / N_t * right_impurity\n","                            - N_t_L / N_t * left_impurity)\n","\n","    where ``N`` is the total number of samples, ``N_t`` is the number of\n","    samples at the current node, ``N_t_L`` is the number of samples in the\n","    left child, and ``N_t_R`` is the number of samples in the right child.\n","\n","    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n","    if ``sample_weight`` is passed.\n","\n","    .. versionadded:: 0.19\n","\n","bootstrap : bool, default=True\n","    Whether bootstrap samples are used when building trees. If False, the\n","    whole dataset is used to build each tree.\n","\n","oob_score : bool, default=False\n","    Whether to use out-of-bag samples to estimate the generalization score.\n","    Only available if bootstrap=True.\n","\n","n_jobs : int, default=None\n","    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n","    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n","    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n","    context. ``-1`` means using all processors. See :term:`Glossary\n","    <n_jobs>` for more details.\n","\n","random_state : int, RandomState instance or None, default=None\n","    Controls both the randomness of the bootstrapping of the samples used\n","    when building trees (if ``bootstrap=True``) and the sampling of the\n","    features to consider when looking for the best split at each node\n","    (if ``max_features < n_features``).\n","    See :term:`Glossary <random_state>` for details.\n","\n","verbose : int, default=0\n","    Controls the verbosity when fitting and predicting.\n","\n","warm_start : bool, default=False\n","    When set to ``True``, reuse the solution of the previous call to fit\n","    and add more estimators to the ensemble, otherwise, just fit a whole\n","    new forest. See :term:`Glossary <warm_start>` and\n","    :ref:`gradient_boosting_warm_start` for details.\n","\n","class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n","    Weights associated with classes in the form ``{class_label: weight}``.\n","    If not given, all classes are supposed to have weight one. For\n","    multi-output problems, a list of dicts can be provided in the same\n","    order as the columns of y.\n","\n","    Note that for multioutput (including multilabel) weights should be\n","    defined for each class of every column in its own dict. For example,\n","    for four-class multilabel classification weights should be\n","    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n","    [{1:1}, {2:5}, {3:1}, {4:1}].\n","\n","    The \"balanced\" mode uses the values of y to automatically adjust\n","    weights inversely proportional to class frequencies in the input data\n","    as ``n_samples / (n_classes * np.bincount(y))``\n","\n","    The \"balanced_subsample\" mode is the same as \"balanced\" except that\n","    weights are computed based on the bootstrap sample for every tree\n","    grown.\n","\n","    For multi-output, the weights of each column of y will be multiplied.\n","\n","    Note that these weights will be multiplied with sample_weight (passed\n","    through the fit method) if sample_weight is specified.\n","\n","ccp_alpha : non-negative float, default=0.0\n","    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n","    subtree with the largest cost complexity that is smaller than\n","    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n","    :ref:`minimal_cost_complexity_pruning` for details.\n","\n","    .. versionadded:: 0.22\n","\n","max_samples : int or float, default=None\n","    If bootstrap is True, the number of samples to draw from X\n","    to train each base estimator.\n","\n","    - If None (default), then draw `X.shape[0]` samples.\n","    - If int, then draw `max_samples` samples.\n","    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n","      `max_samples` should be in the interval `(0.0, 1.0]`.\n","\n","    .. versionadded:: 0.22\n","\n","Attributes\n","----------\n","estimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\n","    The child estimator template used to create the collection of fitted\n","    sub-estimators.\n","\n","    .. versionadded:: 1.2\n","       `base_estimator_` was renamed to `estimator_`.\n","\n","base_estimator_ : DecisionTreeClassifier\n","    The child estimator template used to create the collection of fitted\n","    sub-estimators.\n","\n","    .. deprecated:: 1.2\n","        `base_estimator_` is deprecated and will be removed in 1.4.\n","        Use `estimator_` instead.\n","\n","estimators_ : list of DecisionTreeClassifier\n","    The collection of fitted sub-estimators.\n","\n","classes_ : ndarray of shape (n_classes,) or a list of such arrays\n","    The classes labels (single output problem), or a list of arrays of\n","    class labels (multi-output problem).\n","\n","n_classes_ : int or list\n","    The number of classes (single output problem), or a list containing the\n","    number of classes for each output (multi-output problem).\n","\n","n_features_in_ : int\n","    Number of features seen during :term:`fit`.\n","\n","    .. versionadded:: 0.24\n","\n","feature_names_in_ : ndarray of shape (`n_features_in_`,)\n","    Names of features seen during :term:`fit`. Defined only when `X`\n","    has feature names that are all strings.\n","\n","    .. versionadded:: 1.0\n","\n","n_outputs_ : int\n","    The number of outputs when ``fit`` is performed.\n","\n","feature_importances_ : ndarray of shape (n_features,)\n","    The impurity-based feature importances.\n","    The higher, the more important the feature.\n","    The importance of a feature is computed as the (normalized)\n","    total reduction of the criterion brought by that feature.  It is also\n","    known as the Gini importance.\n","\n","    Warning: impurity-based feature importances can be misleading for\n","    high cardinality features (many unique values). See\n","    :func:`sklearn.inspection.permutation_importance` as an alternative.\n","\n","oob_score_ : float\n","    Score of the training dataset obtained using an out-of-bag estimate.\n","    This attribute exists only when ``oob_score`` is True.\n","\n","oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n","    Decision function computed with out-of-bag estimate on the training\n","    set. If n_estimators is small it might be possible that a data point\n","    was never left out during the bootstrap. In this case,\n","    `oob_decision_function_` might contain NaN. This attribute exists\n","    only when ``oob_score`` is True.\n","\n","See Also\n","--------\n","sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n","sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n","    tree classifiers.\n","\n","Notes\n","-----\n","The default values for the parameters controlling the size of the trees\n","(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n","unpruned trees which can potentially be very large on some data sets. To\n","reduce memory consumption, the complexity and size of the trees should be\n","controlled by setting those parameter values.\n","\n","The features are always randomly permuted at each split. Therefore,\n","the best found split may vary, even with the same training data,\n","``max_features=n_features`` and ``bootstrap=False``, if the improvement\n","of the criterion is identical for several splits enumerated during the\n","search of the best split. To obtain a deterministic behaviour during\n","fitting, ``random_state`` has to be fixed.\n","\n","References\n","----------\n",".. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n","\n","Examples\n","--------\n",">>> from sklearn.ensemble import RandomForestClassifier\n",">>> from sklearn.datasets import make_classification\n",">>> X, y = make_classification(n_samples=1000, n_features=4,\n","...                            n_informative=2, n_redundant=0,\n","...                            random_state=0, shuffle=False)\n",">>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",">>> clf.fit(X, y)\n","RandomForestClassifier(...)\n",">>> print(clf.predict([[0, 0, 0, 0]]))\n","[1]\n","\u001b[0;31mFile:\u001b[0m           ~/.python/current/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\n","\u001b[0;31mType:\u001b[0m           ABCMeta\n","\u001b[0;31mSubclasses:\u001b[0m     \n"]}],"source":["RandomForestClassifier?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Definimos los parámetros de búsqueda\n","params = {\n","    'n_estimators' : [50,100],\n","    'max_features': [2,4,5],\n","    'criterion': ['gini'], \n","    'max_depth':[4,5]\n","}\n","\n","grid_random_forest = GridSearchCV(estimator = random_forest,\n","                                  param_grid = params,\n","                                  scoring = 'neg_mean_absolute_error',  # Opciones:r2, explained_variance\n","                                  cv = 5, \n","                                  verbose = 1, #Muestra el resultado en pantalla\n","                                  n_jobs = -1) # corrida en paralelo"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 12 candidates, totalling 60 fits\n","CPU times: user 2.78 s, sys: 124 ms, total: 2.9 s\n","Wall time: 37.6 s\n"]},{"data":{"text/plain":["{'criterion': 'gini', 'max_depth': 5, 'max_features': 4, 'n_estimators': 100}"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","#Entrenamos el modelo \n","grid_random_forest.fit(x_train, y_train)\n","grid_random_forest.best_params_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=5, max_features=5, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=5, max_features=5, random_state=42)</pre></div></div></div></div></div>"],"text/plain":["RandomForestClassifier(max_depth=5, max_features=5, random_state=42)"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["clf = RandomForestClassifier(max_depth=5, random_state=42, n_estimators=100,criterion='gini', max_features = 5)\n","clf.fit(x_train, y_train)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Metricas"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn import metrics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["array([5581, 1446, 1601, 2511])"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["preds = clf.predict(x_test) \n","confusion = metrics.confusion_matrix(y_test, preds)\n","confusion.ravel()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Presicion"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["0.7264565939491875"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["accuracy = metrics.accuracy_score(y_test, preds)\n","accuracy "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["(0.6345716451857468, 0.7770815928710666)"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["precision_positiva = metrics.precision_score(y_test, preds, pos_label=1)\n","precision_negativa = metrics.precision_score(y_test, preds, pos_label=0)\n","precision_positiva, precision_negativa "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Sensibilidad y especificidad"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["(0.6106517509727627, 0.7942222854703287)"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["recall_sensibilidad = metrics.recall_score(y_test, preds, pos_label=1)\n","recall_especificidad= metrics.recall_score(y_test, preds, pos_label=0)\n","recall_sensibilidad, recall_especificidad"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["(0.6223819556326683, 0.7855584488704341)"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["f1_positivo = metrics.f1_score(y_test, preds, pos_label=1)\n","f1_negativo = metrics.f1_score(y_test, preds, pos_label=0)\n","f1_positivo, f1_negativo "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         0.0       0.78      0.79      0.79      7027\n","         1.0       0.63      0.61      0.62      4112\n","\n","    accuracy                           0.73     11139\n","   macro avg       0.71      0.70      0.70     11139\n","weighted avg       0.72      0.73      0.73     11139\n","\n"]},{"name":"stderr","output_type":"stream","text":["Bad pipe message: %s [b'<\\xfeU\\xa6\\xb3\\xf8\\xd6\\xec\\xa6\\x1b\\xf1g\\x90\\xda$\\xe3\\xf4']\n","Bad pipe message: %s [b'\\xd6\\xc2%}\\xden\\xa2\\xbd3\\xc7,\\xb8dX\\x10I\\xc1L L\\x17\\x82\\n\\xacr\\xfe\\x93]Z\\x05V\\xc2\\x9a\\xbf\\xb2\\x7f\\xda\\x81\\x13\\xb6E\\xf5Q\\xc7}\\xad\\xfb\\x0b\\xf8\\xb4\\x15\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00', b'\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e']\n","Bad pipe message: %s [b\"\\xad+\\x1c\\x9a\\x81?Z\\xe2\\xe7`.Q\\x91\\xa7\\xa8\\x90\\x89\\xb6\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\"]\n","Bad pipe message: %s [b'r-|Ho\\nXn\\xbb\\xb4\\xc0\\x7f\\xa6\\x8f\\xfc\\xab\\x91M\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e']\n","Bad pipe message: %s [b'\\xb9\\x1a\\xff&d\\x0b\\xdf\\x9b\\xd2\\xa2\\xaa\\x01$\\x03\\xf9\\xf8\\xbfK\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00']\n","Bad pipe message: %s [b'\\x06\\x00\\x17\\x00\\x03\\xc0\\x10']\n","Bad pipe message: %s [b'h\\xe6)\\xfe\\xcc\\xd9\\tm\\xc2\\x14\\x10\\x03\\x1dW\\xd2\\xcc\\xe2\\xfd\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0']\n","Bad pipe message: %s [b'\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03']\n","Bad pipe message: %s [b'\\xba~\\x94\\x86\\x88\\xba\\x8b\\x7f~D\\x90\\x98\\x80u\\x84\\xd9\\xab\\x8b\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00']\n","Bad pipe message: %s [b\"\\xe3}\\x17\\xb5\\x01\\x0b\\x01\\xc7\\xcc2\\xa8\\xb9\\xb0\\x93\\x94\\xce\\xab\\x89\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\", b'#\\x00\\x00\\x00\\r\\x00 \\x00\\x1e']\n","Bad pipe message: %s [b'\\x06\\x02\\x06\\x03\\x05', b'', b'\\x03', b'\\x04\\x02\\x04', b'\\x01\\x03', b'\\x03', b'\\x02', b'\\x03']\n","Bad pipe message: %s [b'\\x85l\\x03\\xf2\\x96\\xfd\\xb7\\x99n\\xfc\\x82\\x8d\\xd4t\\xfc\\xdc\\x0e\\xe6\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/']\n"]}],"source":["# Todas las metricas en uno\n","print(metrics.classification_report(y_test, preds))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Conclusion preliminar"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Luego de las pruebas realizadas con Random forest se puede llegar a pensar que el arbol de decicion produjo mejores resultados. Faltan hacer pruebas con mejor tratamiento de Outlayers y de scalamiento. "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"}}},"nbformat":4,"nbformat_minor":2}
